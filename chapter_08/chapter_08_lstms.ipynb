{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
      "metadata": {
        "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b"
      },
      "source": [
        "# Chapter 8 -- Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs)\n",
        "\n",
        "In this tutorial, we will use [PyTorch](https://pytorch.org/) and [Lightning](https://lightning.ai/) to create, optimize and make predictions using the **Long Short-Term Memory (LSTM)** network. We will implement the **LSTM** unit schematized below, that uses predicts sequential data to predict the value of two different companies.\n",
        "\n",
        "<img src=\"./images/lstm_image.001.png\" alt=\"A Long Short-Term Memory Unit\" style=\"width: 800px;\">\n",
        "\n",
        "The training data (below) consist of stock prices for two different companies, *Company A* and *Company B*. The goal is to use the data from the first **4** days to predict what the price will be on the **5**th day. If we look closely at the data, we'll see that the only differences in the prices occur on Day **1** and Day **5**. So the LSTM has to remember what happened on Day **1** in order to predict what will happen on Day **5**.\n",
        "\n",
        "<img src=\"./images/company_data.png\" alt=\"Data for Companies A and B\" style=\"width: 800px;\">\n",
        "\n",
        "In this tutorial, we will:\n",
        "\n",
        "- Build a Long Short-Term Memory (LSTM) unit by hand with Lightning\n",
        "- Train the LSTM unit and use Lightning and TensorBoard to evaluate\n",
        "- Add additional epochs to the training without starting over\n",
        "- Build a Long Short-Term Memory Unit with nn.LSTM() and train it with Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
      "metadata": {
        "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "import lightning as L\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# from pytorch_lightning.loggers import TensorBoardLogger  # logger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0",
      "metadata": {
        "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0"
      },
      "source": [
        "## Build a Long Short-Term Memory unit by hand\n",
        "\n",
        "Just like we have done in previous tutorials, building a neural network, and a Long Short-Term Memory (LSTM) unit is a type of neural network, means we need to create a new class. To make it easy to train the LSTM, this class will inherit from `LightningModule` and we'll create the following methods:\n",
        "\n",
        "- `__init__()` to initialize the Weights and Biases and keep track of a few other house keeping things.\n",
        "- `lstm_unit()` to do the LSTM math. For example, to calculate the percentage of the long-term memory to remember.\n",
        "- `forward()` to make a forward pass through the unrolled LSTM. In other words `forward()` calls `lstm_unit()` for each data point.\n",
        "- `configure_optimizers()` to configure the opimimizer. In this tutorial we'll use `Adam`, another popular algorithm for optimizing the Weights and Biases.\n",
        "- `training_step()` to pass the training data to `forward()`, calculate the loss and to keep track of the loss values in a log file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b4274850-fac0-4099-87ce-342d0aab2fab",
      "metadata": {
        "id": "b4274850-fac0-4099-87ce-342d0aab2fab"
      },
      "outputs": [],
      "source": [
        "class LSTMbyHand(L.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "        L.seed_everything(seed=42)\n",
        "\n",
        "        # nn.LSTM() uses random values from a uniform distribution to initialize the tensors\n",
        "        # Here we can do it 2 different ways 1) Normal Distribution and 2) Uniform Distribution\n",
        "        # We'll start with the Normal Distribtion using `torch.normal()`\n",
        "        mean = torch.tensor(0.0)\n",
        "        std = torch.tensor(1.0)\n",
        "\n",
        "        # In this case, we are only using the normal distribution for the Weights.\n",
        "        # All Biases are initialized to 0.\n",
        "\n",
        "        # These are the Weights and Biases in the first stage, which determines what percentage\n",
        "        # of the long-term memory (blue cell, or *forget gate*) the LSTM unit will remember.\n",
        "        self.wlr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.wlr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
        "\n",
        "        # These are the Weights and Biases in the second stage, which determins the new\n",
        "        # potential long-term memory (*input gate*) and what percentage will be remembered.\n",
        "        self.wpr1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.wpr2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
        "\n",
        "        self.wp1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.wp2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
        "\n",
        "        # These are the Weights and Biases in the third stage, which determines the\n",
        "        # new short-term memory (*output gate*) and what percentage will be sent to the output.\n",
        "        self.wo1 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.wo2 = nn.Parameter(torch.normal(mean=mean, std=std), requires_grad=True)\n",
        "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
        "\n",
        "        # # We can also initialize all Weights and Biases using a uniform distribution:\n",
        "        # self.wlr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.wlr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.blr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "\n",
        "        # self.wpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.wpr2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.bpr1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "\n",
        "        # self.wp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.wp2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.bp1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "\n",
        "        # self.wo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.wo2 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "        # self.bo1 = nn.Parameter(torch.rand(1), requires_grad=True)\n",
        "\n",
        "\n",
        "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
        "        # lstm_unit does the math for a single LSTM unit.\n",
        "\n",
        "        # long term memory is also called \"cell state\",\n",
        "        # short term memory is also called \"hidden state\"\n",
        "\n",
        "        # 1) The first stage determines what percent of the current\n",
        "        # long-term memory should be remembered\n",
        "        long_remember_percent = torch.sigmoid(\n",
        "            (short_memory * self.wlr1)\n",
        "            +\n",
        "            (input_value * self.wlr2)\n",
        "            +\n",
        "            self.blr1\n",
        "        )\n",
        "\n",
        "        # 2) The second stage creates a new, potential long-term memory\n",
        "        # and determines what percentage of that to add to the current long-term memory\n",
        "        potential_remember_percent = torch.sigmoid(\n",
        "            (short_memory * self.wpr1)\n",
        "            +\n",
        "            (input_value * self.wpr2)\n",
        "            +\n",
        "            self.bpr1\n",
        "        )\n",
        "        \n",
        "        potential_memory = torch.tanh(\n",
        "            (short_memory * self.wp1)\n",
        "            +\n",
        "            (input_value * self.wp2)\n",
        "            +\n",
        "            self.bp1\n",
        "        )\n",
        "\n",
        "        # Once we have gone through the first two stages, we can update the long-term memory\n",
        "        updated_long_memory = (\n",
        "            (long_memory * long_remember_percent)\n",
        "            +\n",
        "            (potential_remember_percent * potential_memory)\n",
        "        )\n",
        "\n",
        "        # 3) The third stage creates a new, potential short-term memory and determines\n",
        "        # what percentage of that should be remembered and used as output.\n",
        "        output_percent = torch.sigmoid(\n",
        "            (short_memory * self.wo1)\n",
        "            +\n",
        "            (input_value * self.wo2)\n",
        "            +\n",
        "            self.bo1\n",
        "        )\n",
        "\n",
        "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
        "\n",
        "        # Finally, we return the updated long and short-term memories\n",
        "        return([updated_long_memory, updated_short_memory])\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        # forward() unrolls the LSTM for the training data by calling lstm_unit()\n",
        "        # for each day of training data that we have. forward() also keeps track of\n",
        "        # the long and short-term memories after each day and returns the final \n",
        "        # short-term memory, which is the 'output' of the LSTM.\n",
        "\n",
        "        long_memory = 0  # long term memory is also called \"cell state\" and indexed with c0, c1, ..., cN\n",
        "        short_memory = 0  # short term memory is also called \"hidden state\" and indexed with h0, h1, ..., cN\n",
        "        \n",
        "        day1 = input[0]\n",
        "        day2 = input[1]\n",
        "        day3 = input[2]\n",
        "        day4 = input[3]\n",
        "\n",
        "        # Day 1\n",
        "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
        "\n",
        "        # Day 2\n",
        "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
        "\n",
        "        # Day 3\n",
        "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
        "\n",
        "        # Day 4\n",
        "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
        "\n",
        "        # Now return short_memory, which is the 'output' of the LSTM.\n",
        "        return short_memory\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # this configures the optimizer we want to use for backpropagation.\n",
        "\n",
        "        # Setting the learning rate to 0.1 trains way faster than using the \n",
        "        # default learning rate, lr=0.001, which requires a lot more training. \n",
        "        # However, if we use the default value, we get the exact same Weights \n",
        "        # and Biases that was used in the LSTM Clearly Explained StatQuest video. \n",
        "        # So we'll use the default value.\n",
        "        # return Adam(self.parameters(), lr=0.1)\n",
        "        return Adam(self.parameters())\n",
        "\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        # take a step during gradient descent.\n",
        "        input_i, label_i = batch  # collect input\n",
        "        output_i = self.forward(input_i[0])  # run input through the neural network\n",
        "        loss = (output_i - label_i)**2  # loss = squared residual\n",
        "\n",
        "        ###################\n",
        "        ## Logging the loss and the predicted values so we can evaluate the training\n",
        "        ###################\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "        \n",
        "        # Our dataset consists of two sequences of values representing Company A and \n",
        "        # Company B. For Company A, the goal is to predict that the value on Day 5 = 0, \n",
        "        # and for Company B, the goal is to predict that the value on Day 5 = 1. We use \n",
        "        # label_i, the value we want to predict, to keep track of which company we just \n",
        "        # made a prediction for and log that output value in a company specific file\n",
        "        if (label_i == 0):\n",
        "            self.log(\"out_0\", output_i)\n",
        "        else:\n",
        "            self.log(\"out_1\", output_i)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb",
      "metadata": {
        "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb"
      },
      "source": [
        "Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized Weights and Biases.\n",
        "\n",
        "Then, just for fun, we'll see what those random Weights and Biases predict for *Company A* and *Company B*. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e3ea653-f746-46db-9029-44242a1b8391",
      "metadata": {
        "id": "4e3ea653-f746-46db-9029-44242a1b8391"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before optimization, the parameters are:\n",
            "wlr1 tensor(0.3367)\n",
            "wlr2 tensor(0.1288)\n",
            "blr1 tensor(0.)\n",
            "wpr1 tensor(0.2345)\n",
            "wpr2 tensor(0.2303)\n",
            "bpr1 tensor(0.)\n",
            "wp1 tensor(-1.1229)\n",
            "wp2 tensor(-0.1863)\n",
            "bp1 tensor(0.)\n",
            "wo1 tensor(2.2082)\n",
            "wo2 tensor(-0.6380)\n",
            "bo1 tensor(0.)\n",
            "\n",
            "Now let's compare the observed and predicted values:\n",
            "Company A: Observed = 0, Predicted = tensor(-0.0377)\n",
            "Company B: Observed = 1, Predicted = tensor(-0.0383)\n"
          ]
        }
      ],
      "source": [
        "# Create the model object, print out parameters and see how well\n",
        "# the untrained LSTM can make predictions...\n",
        "model = LSTMbyHand()\n",
        "\n",
        "print(\"Before optimization, the parameters are:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)\n",
        "\n",
        "print(\"\\nNow let's compare the observed and predicted values:\")\n",
        "\n",
        "# To make predictions, we pass in the first 4 days worth of stock \n",
        "# values in an array for each company. In this case, the only difference \n",
        "# between the input values for Company A and B occurs on the first day. \n",
        "# Company A has 0 and Company B has 1.\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(\n",
        "        torch.tensor([0., 0.5, 0.25, 1.])\n",
        "    ).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(\n",
        "        torch.tensor([1., 0.5, 0.25, 1.])\n",
        "    ).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1329d6f2-39ff-4f9b-bc99-fbd1a2013815",
      "metadata": {
        "id": "1329d6f2-39ff-4f9b-bc99-fbd1a2013815"
      },
      "source": [
        "With the unoptimized paramters, the predicted value for *Company A*, **-0.0377**, isn't terrible, since it is relatively close to the observed value, **0**. However, the predicted value for *Company B*, **-0.0383**, _is_ terrible, because it is relatively far from the observed value, **1**. So, that means we need to train the LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "784e2424-649a-44ce-8a34-c98b8fced609",
      "metadata": {
        "id": "784e2424-649a-44ce-8a34-c98b8fced609"
      },
      "source": [
        "## LSTM with Lightning and TensorBoard\n",
        "\n",
        "### Getting Started\n",
        "\n",
        "Since we are using **Lightning** training, training the LSTM we created by hand is pretty easy. All we have to do is create the training data and put it into a `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
      "metadata": {
        "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1"
      },
      "outputs": [],
      "source": [
        "# Create the training data for the neural network.\n",
        "inputs = torch.tensor(\n",
        "    [\n",
        "        [0., 0.5, 0.25, 1.],\n",
        "        [1., 0.5, 0.25, 1.]\n",
        "    ])\n",
        "labels = torch.tensor([0., 1.])\n",
        "\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ea2ef9-7fe2-4bfe-8645-4db921a4b20c",
      "metadata": {
        "id": "e6ea2ef9-7fe2-4bfe-8645-4db921a4b20c"
      },
      "source": [
        "We then create a **Lightning Trainer**, `L.Trainer`, and fit it to the training data. We will be starting with **2000** epochs. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out after we compare the predictions to the observed values.\n",
        "\n",
        "Since we'll be using TensorBoard to visualize the training progress, we need to ensure the correct logger is used. By default (as long as the `tensorboard` package is installed), PyTorch `Lightning` is smart and **automatically uses the `TensorBoardLogger`**.Though, to have more control over our experiment (like setting a custom name for our run), we can **explicitly** create and pass the `TensorBoardLogger` to the **Trainer** using the `logger` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "10157397-3100-4d18-be96-cdc294ce89e0",
      "metadata": {
        "id": "10157397-3100-4d18-be96-cdc294ce89e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name         | Type | Params | Mode\n",
            "---------------------------------------------\n",
            "  | other params | n/a  | 12     | n/a \n",
            "---------------------------------------------\n",
            "12        Trainable params\n",
            "0         Non-trainable params\n",
            "12        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2557e6dc7c294925b591920d9bc99e7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(\n",
        "    max_epochs=2000,  # with default learning rate, 0.001\n",
        "    # logger=logger,    # pass another logger\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2586093b-38e5-431a-a7f0-ad65f2a4c979",
      "metadata": {
        "id": "2586093b-38e5-431a-a7f0-ad65f2a4c979"
      },
      "source": [
        "Now that we've trained the model with **2000** epochs, let's see how good the predictions are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a0eb6a44",
      "metadata": {
        "id": "a0eb6a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observed and predicted values (after 2000 epochs; lr=0.001):\n",
            "Company A: Observed = 0, Predicted = tensor(0.4342)\n",
            "Company B: Observed = 1, Predicted = tensor(0.6171)\n"
          ]
        }
      ],
      "source": [
        "print(\"New observed and predicted values (after 2000 epochs; lr=0.001):\")\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a292d775-336f-41b7-838c-b297c49693a8",
      "metadata": {
        "id": "a292d775-336f-41b7-838c-b297c49693a8"
      },
      "source": [
        "### TensorBoard\n",
        "\n",
        "Unfortunately, these predictions are terrible. So it seems like we'll have to do more training. However, it would be awesome if we could be confident that more training will actually improve the predictions. If not, we can spare ourselves a lot of time, and potentially money, and just give up.\n",
        "\n",
        "Before we dive into more training, let's look at the loss values and predictions that we saved in log files with **TensorBoard**. **TensorBoard** will graph everything that we logged during training, making it super easy to see if things are headed in the right direction or not.\n",
        "\n",
        "We first need to make sure that **TensorBoard** is installed. If it isn't, we can install it by following the [instructions to use TensorBoard with PyTorch](https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e182b3e6",
      "metadata": {},
      "source": [
        "Then, to get TensorBoard working:\n",
        "\n",
        "- In the Jupyter browser window, go to the **File** menu and select **New**.\n",
        "- In the submenu, select **Terminal**.\n",
        "- In the terminal, navigate to the same directory that contains the **lightning_logs** directory, i.e. to `chapter_08/`.\n",
        "- Then in the terminal, enter `tensorboard --logdir=lightning_logs/` to start the **TensorBoard** server.\n",
        "- When the **TensorBoard** server starts, it will print out a URL that looks like this `http://localhost:6006/`.\n",
        "- Copy the URL and paste it into a new browser window and then we are good to go.\n",
        "\n",
        "Alternatively, we can run the server in a Jupyter notebook cell by entering the following command:\n",
        "\n",
        "```python\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# Start TensorBoard, pointing it to the log directory\n",
        "%tensorboard --logdir lightning_logs\n",
        "```\n",
        "\n",
        "Though currently we may encounter some issue with the connection to the TensorBoard server at kernel restart, [as explained elsewhere](https://github.com/tensorflow/tensorboard/issues/2481).\n",
        "\n",
        "_If the graphs look messed up and we see a bunch of different lines, instead of just one red line per graph, then check where this notebook is saved for a directory called `lightning_logs`. Delete `lightning_logs` and the re-run everything in this notebook. One source of problems with the graphs is that every time we train a model, a new batch of log files is created and stored in `lightning_logs` and **TensorBoard**, by default, will plot all of them. We can turn off unwanted log files in **TensorBoard**, and we'll do this later on in this notebook, but for now, the easiest thing to do is to start with a clean slate._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9ccf9b",
      "metadata": {},
      "source": [
        "Below are the graphs of **loss** (`train_loss`), the predictions for *Company A* (`out_0`), and the predictions for *Company B* (`out_1`). Remember for *Companay A*, we want to predict **0** and for *Company B*, we want to predict **1**.\n",
        "\n",
        "<img src=\"./images/train_loss_2000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_2000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_2000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
        "\n",
        "If we look at the **loss** (`train_loss`), we see that it is going down, which is good, but it still has further to go. When we look at the predictions for *Company A* (`out_0`), we see that they started out pretty good, close to **0**, but then got really bad early on in training, shooting all the way up to **0.5**, but are starting to get smaller. In contrast, when we look at the predictions for *Company B* (`out_1`), we see that they started out really bad, close to **0**, but have been getting better ever since and look like they could continue to get better if we kept training.\n",
        "\n",
        "In summary, the graphs seem to suggest that if we continued training our model, the predictions would improve. So let's add more epochs to the training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4989ee2-f2ce-40c1-b8a6-0f3c4d4e06c6",
      "metadata": {
        "id": "c4989ee2-f2ce-40c1-b8a6-0f3c4d4e06c6"
      },
      "source": [
        "## Adding More Epochs\n",
        "\n",
        "The good news is that because we're using **Lightning**, we can pick up where we left off training without having to start over from scratch. This is because when we train with **Lightning**, it creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are located. This will save us a lot of time since we don't have to retrain the first **2000** epochs. So let's add an additional **1000** epochs to the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "10ff55c1-cdc2-4086-b0d4-8d8333058674",
      "metadata": {
        "id": "10ff55c1-cdc2-4086-b0d4-8d8333058674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The new trainer will start where the last left off:\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_08\\lightning_logs\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n"
          ]
        }
      ],
      "source": [
        "# First, find where the most recent checkpoint files are stored\n",
        "path_to_checkpoint = trainer.checkpoint_callback.best_model_path  # By default, \"best\" = \"most recent\"\n",
        "\n",
        "print(\n",
        "    \"The new trainer will start where the last left off:\",\n",
        "    path_to_checkpoint,\n",
        "    sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035ee1c8",
      "metadata": {},
      "source": [
        "This will also create a new subfolder within the `lightning_logs` folder, more precisely a new \"version\" which will be concatenated in TensorBoard. We can track this added data by changing the graph color for each version. Don't forget to \"fit domain to data\" when analyzing the graphs on TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "25ed2f74",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Restoring states from the checkpoint path at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_08\\lightning_logs\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:445: The dirpath has changed from 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_08\\\\lightning_logs\\\\version_0\\\\checkpoints' to 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_08\\\\lightning_logs\\\\version_1\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "\n",
            "  | Name         | Type | Params | Mode\n",
            "---------------------------------------------\n",
            "  | other params | n/a  | 12     | n/a \n",
            "---------------------------------------------\n",
            "12        Trainable params\n",
            "0         Non-trainable params\n",
            "12        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_08\\lightning_logs\\version_0\\checkpoints\\epoch=1999-step=4000.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa08dc30c16d4e35926bd1276670ce39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=3000` reached.\n"
          ]
        }
      ],
      "source": [
        "# Then create a new Lightning Trainer\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=3000,  # by setting it to 3000, we're adding 1000 more\n",
        "    # logger=logger,\n",
        ")\n",
        "\n",
        "# And then call fit() using the path to the most recent checkpoint files\n",
        "# so that we can pick up where we left off\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=dataloader,\n",
        "    ckpt_path=path_to_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b6f6af6-7d7e-4c40-9bbf-f467114d838e",
      "metadata": {
        "id": "3b6f6af6-7d7e-4c40-9bbf-f467114d838e"
      },
      "source": [
        "Now that we have added **1000** epochs to the training, let's check the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2c805e2b-27ae-4302-af26-5fe0d55f6b5b",
      "metadata": {
        "id": "2c805e2b-27ae-4302-af26-5fe0d55f6b5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observed and predicted values (after 3000 total epochs):\n",
            "Company A: Observed = 0, Predicted = tensor(0.2708)\n",
            "Company B: Observed = 1, Predicted = tensor(0.7534)\n"
          ]
        }
      ],
      "source": [
        "print(\"New observed and predicted values (after 3000 total epochs):\")\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42455342-f774-43a7-a043-536099d39b01",
      "metadata": {
        "id": "42455342-f774-43a7-a043-536099d39b01"
      },
      "source": [
        "They are much better than before. We can also check the logs with **TensorBoard** to see if it makes sense to add more epochs to the training (also note that we can set logarithmic scale).\n",
        "\n",
        "Since we already have **TensorBoard** running in a separate browser window, all we have to do is reload that page to update the graphs (below).\n",
        "\n",
        "<img src=\"./images/train_loss_3000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_3000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_3000_epochs.png?raw=1\" alt=\"out_1\" style=\"width: 300px;\">\n",
        "\n",
        "The blue lines in each graph represents the values we logged during the extra **1000** epochs. The **loss** is getting smaller and the predictions for both companies are improving!\n",
        "\n",
        "However, because it looks like there is even more room for improvement, let's add **2000** more epochs to the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7fb14b21-2aa4-4d37-a58c-d2fb789c06c5",
      "metadata": {
        "id": "7fb14b21-2aa4-4d37-a58c-d2fb789c06c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Restoring states from the checkpoint path at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_08\\lightning_logs\\version_1\\checkpoints\\epoch=2999-step=6000.ckpt\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:445: The dirpath has changed from 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_08\\\\lightning_logs\\\\version_1\\\\checkpoints' to 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_08\\\\lightning_logs\\\\version_2\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "\n",
            "  | Name         | Type | Params | Mode\n",
            "---------------------------------------------\n",
            "  | other params | n/a  | 12     | n/a \n",
            "---------------------------------------------\n",
            "12        Trainable params\n",
            "0         Non-trainable params\n",
            "12        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "0         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_08\\lightning_logs\\version_1\\checkpoints\\epoch=2999-step=6000.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bacd1758ddb84a48a91dbd86323d095e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
          ]
        }
      ],
      "source": [
        "path_to_checkpoint = trainer.checkpoint_callback.best_model_path\n",
        "\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=5000,  # By setting it to 5000, we're adding 2000 more\n",
        "    # logger=logger,\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=dataloader,\n",
        "    ckpt_path=path_to_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "231699e4-56eb-4c1b-b48c-e146b8d424fd",
      "metadata": {
        "id": "231699e4-56eb-4c1b-b48c-e146b8d424fd"
      },
      "source": [
        "Now that we have added **2000** more epochs to the training (for a total of **5000** epochs), let's check the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "28c653dc-54ed-4c1a-aca0-419e2311f75e",
      "metadata": {
        "id": "28c653dc-54ed-4c1a-aca0-419e2311f75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observed and predicted values (after 5000 total epochs):\n",
            "Company A: Observed = 0, Predicted = tensor(0.0022)\n",
            "Company B: Observed = 1, Predicted = tensor(0.9693)\n"
          ]
        }
      ],
      "source": [
        "print(\"New observed and predicted values (after 5000 total epochs):\")\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4b82e-6635-4dcd-b889-8374ecb93dad",
      "metadata": {
        "id": "2db4b82e-6635-4dcd-b889-8374ecb93dad"
      },
      "source": [
        "And they look good! The prediction for *Company A* is super close to **0**, which is exactly what we want, and the prediction for *Company B* is close to **1**, which is also what we want.\n",
        "\n",
        "Now let's look at the graphs in **TensorBoard** by updating the page (though we can set an automatic reload of the data)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7741dfd",
      "metadata": {},
      "source": [
        "<img src=\"./images/train_loss_5000_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"> <img src=\"./images/out_0_5000_epochs.png\" alt=\"out_0\" style=\"width: 300px;\"> <img src=\"./images/out_1_5000_epochs.png\" alt=\"out_1\" style=\"width: 300px;\">\n",
        "\n",
        "The dark red lines show how things changed when we added an additional **2000** epochs to the training, for a total of **5000** epochs. Now we see that the **loss** (`train_loss`) and the predictions for each company apper to be tapering off, suggesting that adding more epochs may not improve the predictions much, so we're done.\n",
        "\n",
        "Lastly, let's print out the final estimates for the Weights and Biases. In theory, they should be the same (within rounding error) as what we used in the **StatQuest** on **Long Short-Term Memory** and seen in the diagram of the **LSTM** unit at the top of this Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "64f4e6b4-7f55-4946-8e76-7a52cf2a39b4",
      "metadata": {
        "id": "64f4e6b4-7f55-4946-8e76-7a52cf2a39b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After optimization, the parameters are:\n",
            "wlr1 tensor(2.7043)\n",
            "wlr2 tensor(1.6307)\n",
            "blr1 tensor(1.6234)\n",
            "wpr1 tensor(1.9983)\n",
            "wpr2 tensor(1.6525)\n",
            "bpr1 tensor(0.6204)\n",
            "wp1 tensor(1.4122)\n",
            "wp2 tensor(0.9393)\n",
            "bp1 tensor(-0.3217)\n",
            "wo1 tensor(4.3848)\n",
            "wo2 tensor(-0.1943)\n",
            "bo1 tensor(0.5935)\n"
          ]
        }
      ],
      "source": [
        "print(\"After optimization, the parameters are:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be30faf-c262-448a-b60d-8455ff9af997",
      "metadata": {
        "id": "8be30faf-c262-448a-b60d-8455ff9af997"
      },
      "source": [
        "## Optimzing the LSTM using `nn.LSTM()`\n",
        "\n",
        "Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let's learn how to take advantage of PyTorch's `nn.LSTM()` function. For the most part, using `nn.LSTM()` allows us to simplify the `__init__()` function and the `forward()` function. The other big difference is that this time, we're not going to try and recreate the parameter values we used so far, and that means we can set the learning rate for the `Adam` to **0.1**. This will speed up training a lot. Everything else stays the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0781c6a1-035f-4149-b509-bf808855fed5",
      "metadata": {
        "id": "0781c6a1-035f-4149-b509-bf808855fed5"
      },
      "outputs": [],
      "source": [
        "class LightningLSTM(L.LightningModule):\n",
        "\n",
        "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
        "\n",
        "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
        "\n",
        "        L.seed_everything(seed=42)\n",
        "\n",
        "        # input_size = number of features (or variables) in the data.\n",
        "        # In our example we only have a single feature (value)\n",
        "        # hidden_size = this determines the dimension of the output,\n",
        "        # in other words, if we set hidden_size=1, then we have 1 output node\n",
        "        # if we set hiddeen_size=50, then we have 50 output nodes (that can then \n",
        "        # be 50 input nodes to a subsequent fully connected neural network.\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_trans = input.view(len(input), 1)  # transpose the input vector\n",
        "        lstm_out, _ = self.lstm(input_trans)\n",
        "\n",
        "        # lstm_out has the short-term memories for all inputs.\n",
        "        # We make our prediction with the last one\n",
        "        prediction = lstm_out[-1]\n",
        "        return prediction\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=0.1)  # we now set the learning rate to 0.1\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        input_i, label_i = batch\n",
        "        output_i = self.forward(input_i[0])\n",
        "        loss = (output_i - label_i)**2\n",
        "\n",
        "        self.log(\"train_loss\", loss)\n",
        "\n",
        "        # if (label_i == 0):\n",
        "        #     self.log(\"out_0\", output_i)\n",
        "        # else:\n",
        "        #     self.log(\"out_1\", output_i)\n",
        "        self.log(\"out_1\", output_i) if label_i else self.log(\"out_0\", output_i)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5899c78-1e93-45f3-96a5-c17654f507cc",
      "metadata": {
        "id": "b5899c78-1e93-45f3-96a5-c17654f507cc"
      },
      "source": [
        "Now let's create the model and print out the initial Weights and Biases and predictinos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0",
      "metadata": {
        "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before optimization, the parameters are:\n",
            "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
            "        [ 0.8300],\n",
            "        [-0.2343],\n",
            "        [ 0.9186]])\n",
            "lstm.weight_hh_l0 tensor([[-0.2191],\n",
            "        [ 0.2018],\n",
            "        [-0.4869],\n",
            "        [ 0.5873]])\n",
            "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
            "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
            "\n",
            "Observed and predicted values:\n",
            "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
            "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
          ]
        }
      ],
      "source": [
        "model = LightningLSTM()\n",
        "\n",
        "print(\"Before optimization, the parameters are:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)\n",
        "\n",
        "print(\"\\nObserved and predicted values:\")\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24a1cf65-09ca-4e94-b629-e606c6030953",
      "metadata": {
        "id": "24a1cf65-09ca-4e94-b629-e606c6030953"
      },
      "source": [
        "As expected, the predictions are bad, so we will train the model.\n",
        "\n",
        "With the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train the model. Now, with the learning rate set to 0.1, we only need 300 epochs. This will influence not only the speed of the training but also the logging steps.\n",
        "\n",
        "_Remind that one **epoch** is one full pass over all of the data. To complete one epoch, the model must see both *Company A* and *Company B*. Actually, our **DataLoader** is set up with a `batch_size=1`. This means a \"step\" consists of processing just one row at a time. Therefore in our setup, **2 steps = 1 epoch**._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d",
      "metadata": {
        "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name | Type | Params | Mode \n",
            "--------------------------------------\n",
            "0 | lstm | LSTM | 16     | train\n",
            "--------------------------------------\n",
            "16        Trainable params\n",
            "0         Non-trainable params\n",
            "16        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "1         Modules in train mode\n",
            "0         Modules in eval mode\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "245ed62a1cb64933ba45bf4723369c15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
          ]
        }
      ],
      "source": [
        "# Because we are doing so few epochs, we have to tell the trainer to log \n",
        "# every 2 steps (or single epoch, since we have two rows of training data), \n",
        "# the default updating the log files is every 50 steps.\n",
        "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
        "\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e23c2b07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After optimization, the parameters are:\n",
            "lstm.weight_ih_l0 tensor([[3.5364],\n",
            "        [1.3869],\n",
            "        [1.5390],\n",
            "        [1.2488]])\n",
            "lstm.weight_hh_l0 tensor([[5.2070],\n",
            "        [2.9577],\n",
            "        [3.2652],\n",
            "        [2.0678]])\n",
            "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
            "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n"
          ]
        }
      ],
      "source": [
        "print(\"After optimization, the parameters are:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22dfd472-5e19-410f-adbe-06ff89ac2efd",
      "metadata": {
        "id": "22dfd472-5e19-410f-adbe-06ff89ac2efd"
      },
      "source": [
        "Now that training is done, let's print out the new predictions..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3",
      "metadata": {
        "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New observed and predicted values (after 300 total epochs; lr=0.1):\n",
            "Company A: Observed = 0, Predicted = tensor([6.8118e-05])\n",
            "Company B: Observed = 1, Predicted = tensor([0.9809])\n"
          ]
        }
      ],
      "source": [
        "print(\"New observed and predicted values (after 300 total epochs; lr=0.1):\")\n",
        "print(\n",
        "    \"Company A: Observed = 0, Predicted =\",\n",
        "    model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
        "print(\n",
        "    \"Company B: Observed = 1, Predicted =\",\n",
        "    model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3",
      "metadata": {
        "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3"
      },
      "source": [
        "As we can see, after just **300** epochs, the LSTM is making great predictions. The prediction for *Company A* is close to the observed value **0** and the prediction for *Company B* is close to the observed value **1**.\n",
        "\n",
        "Lastly, let's refresh the **TensorBoard** page to see the latest graphs.\n",
        "\n",
        "_To make it easier to see what we just did, deselect `version_0`, `version_1` and `version_2` and make sure `version_3` is checked on the left-hand side of the page, under where it says `Runs`. See below. This allows us to just look at the log files from the most rescent training, which only went for **300** epochs._\n",
        "\n",
        "<img src=\"./images/selecting_run_version_3.png\" alt=\"Loss\" style=\"width: 150px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5de8c8",
      "metadata": {},
      "source": [
        "In all three graphs, the loss (`train_loss`) and the predictions for *Company A* (`out_0`) and *Company B* (`out_1`) started to taper off after **500** steps, or just **250** epochs, suggesting that adding more epochs may not improve the predictions much, so we're done.\n",
        "\n",
        "<img src=\"./images/train_loss_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_0_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_1_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
