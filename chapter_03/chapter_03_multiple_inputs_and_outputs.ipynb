{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
      "metadata": {
        "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b"
      },
      "source": [
        "# Chapter 3 -- Neural Networks with Multiple Inputs and Outputs\n",
        "\n",
        "In this tutorial, we will use [PyTorch](https://pytorch.org) with [Lightning](https://www.lightning.ai) to create and optimize a simple neural network with multiple inputs and outputs, like the one shown in the picture below:\n",
        "\n",
        "<img src=\"./images/final_nn.png\" alt=\"a neural network with multiple inputs and outputs\" style=\"width: 1000px;\">\n",
        "\n",
        "In this tutorial, we will:\n",
        "\n",
        "- **Import and format data and then build a DataLoader from scratch**\n",
        "- **Build a Neural Network with multiple inputs and outputs**\n",
        "- **Train a Neural Network with multiple inputs and outputs**\n",
        "- **Make predictions with new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
      "metadata": {
        "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import lightning as L\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd  # Read in the data and normalize it\n",
        "from sklearn.model_selection import train_test_split  # Create training and testing datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23672f4-c30e-42de-9d35-5fa6b9bfd0f5",
      "metadata": {
        "id": "d23672f4-c30e-42de-9d35-5fa6b9bfd0f5"
      },
      "source": [
        "## Buil a DataLoader\n",
        "\n",
        "### The Iris Flower dataset\n",
        "\n",
        "Once we have the Python modules imported, we need to import the data that we will use to train and test our neural network. Specifically, we're going to use the **[Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)**, which we will import from a comma-separated (CSV) text file so that we can learn how to build a DataLoader from scratch.\n",
        "\n",
        "It is a classic dataset originally made famous by Rondal Fisher in 1936, and has since been used countless times to demonstrate the effectiveness of various classification algorithms. The dataset consists of 150 samples total, 50 for each of 3 species of Iris, *Setosa*, *Versicolor*, and *Virginica*. Each row in the dataset contains measurements for 4 variables: **[petal](https://en.wikipedia.org/wiki/Petal)** width and length and **[sepal](https://en.wikipedia.org/wiki/Sepal)** width and length.\n",
        "\n",
        "_The data file we are going import, `iris.txt`, was originally downloaded from the **[UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/)**, which has a lot of great datasets that we can practice building Neural Networks (or any other machine learning algorithm) with. When we download the datasets from UCI, we get one file that has the data and another file that describes the data, including providing us with the names of each variable, or column, in the dataset. If we'd like to see the original data files, we can find them **[here](https://archive.ics.uci.edu/dataset/53/iris)**._\n",
        "\n",
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "id": "545b5096-a942-4097-800a-ea38b24c5478",
      "metadata": {
        "id": "545b5096-a942-4097-800a-ea38b24c5478"
      },
      "outputs": [],
      "source": [
        "# We'll read in the dataset with the pandas function read_table()\n",
        "# read_table() can read in various text files including, comma-separated and tab-delimited\n",
        "# url = \"https://raw.githubusercontent.com/StatQuest/signa/main/chapter_03/iris.txt\"\n",
        "# df = pd.read_table(url, sep=\",\", header=None)\n",
        "\n",
        "# Or we can directly use the `read_csv` method\n",
        "df = pd.read_csv(\"iris.txt\", header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb56234a-7c1d-4f62-a655-e3d722ba50c7",
      "metadata": {
        "id": "bb56234a-7c1d-4f62-a655-e3d722ba50c7"
      },
      "source": [
        "Now, in theory, we have loaded the data into a DataFrame called `df`, but it's always a good idea to make sure this worked as expected. So, we'll print out the first handful of rows in the dataset with the `head()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "id": "8f040fe4-dc2c-4258-a438-38d7d975da4e",
      "metadata": {
        "id": "8f040fe4-dc2c-4258-a438-38d7d975da4e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3            4\n",
              "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
              "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
              "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
              "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
              "4  5.0  3.6  1.4  0.2  Iris-setosa"
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print out the first handful of rows using the head() method\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b97bc9-edc4-4915-a498-7a1bb239cd17",
      "metadata": {
        "id": "38b97bc9-edc4-4915-a498-7a1bb239cd17"
      },
      "source": [
        "When we print out the first few rows of our new DataFrame, `df`, the first thing we see is that the columns are not named. In theory, it's fine to have unnamed columns (and just have numbers), but it makes the data hard to look at, so let's add the column names to `df`. To name each column, we simply assign a list of column names to `columns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "id": "2226d80c-defe-4dd7-aa8f-6657346d5279",
      "metadata": {
        "id": "2226d80c-defe-4dd7-aa8f-6657346d5279"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width        class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "execution_count": 256,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To name each column, we assign a list of column names to `columns`\n",
        "df.columns = [\n",
        "    \"sepal_length\",\n",
        "    \"sepal_width\",\n",
        "    \"petal_length\",\n",
        "    \"petal_width\",\n",
        "    \"class\"]\n",
        "\n",
        "# To verify we did that correctly, let's print out the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12738def-82bb-4bc1-b917-d1cf9388d5f8",
      "metadata": {
        "id": "12738def-82bb-4bc1-b917-d1cf9388d5f8"
      },
      "source": [
        "Hooray! Now that we can look at our DataFrame without getting a headache, let's see how big this dataset is and figure out how many different iris species we will have to train our neural network to predict. First, let's see how many rows and columns are in the dataset with `.shape`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "id": "9b875a53-e2dd-481e-98e2-a7983efcba13",
      "metadata": {
        "id": "9b875a53-e2dd-481e-98e2-a7983efcba13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(150, 5)"
            ]
          },
          "execution_count": 257,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape # shape returns the rows and colunns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "id": "08e2732c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   class         150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a20ad34-a948-481f-8f37-663416daeda7",
      "metadata": {
        "id": "3a20ad34-a948-481f-8f37-663416daeda7"
      },
      "source": [
        "So, our dataset has 150 rows and 5 columns. Now let's see how many different types of iris are in it. We'll do this by counting the unique values in the column called `class` with `.nunique()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "id": "c5913c8b-3a8a-4082-8c9d-58eea18607e7",
      "metadata": {
        "id": "c5913c8b-3a8a-4082-8c9d-58eea18607e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 259,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To determine the number of iris species in the dataset,\n",
        "# we'll count the number of unique values in the column called `class`\n",
        "df['class'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7847fb8-c7e9-4ef3-b742-5875dc6de37e",
      "metadata": {
        "id": "e7847fb8-c7e9-4ef3-b742-5875dc6de37e"
      },
      "source": [
        "And we get the number we expected, 3. So that's good! Now let's print out the names of the 3 species with `.unique()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "id": "14ee5364-06d6-4960-9921-23d12c524e8a",
      "metadata": {
        "id": "14ee5364-06d6-4960-9921-23d12c524e8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can print out the unique values in a dataframe's \n",
        "# column with the 'unique()' method\n",
        "df['class'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1401a7fa-1dc8-402f-b4a1-cec544eb0f3e",
      "metadata": {
        "id": "1401a7fa-1dc8-402f-b4a1-cec544eb0f3e"
      },
      "source": [
        "So, just as we expected, we see that we have 3 different species of iris in our dataset: *Setosa*, *Versicolor*, *Virginica*.\n",
        "\n",
        "Now let's verify that our dataset is balanced, meaning we have roughly the same number of entries (rows) in our data for each of the 3 iris species that we want our neural network to classify. We can do this with a fancy `for` loop that prints out the number of rows per class, regardless of the number of classes we have in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "id": "a61c6daa-bc21-4d53-a91f-c48fa4844d77",
      "metadata": {
        "id": "a61c6daa-bc21-4d53-a91f-c48fa4844d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iris-setosa: 50\n",
            "Iris-versicolor: 50\n",
            "Iris-virginica: 50\n"
          ]
        }
      ],
      "source": [
        "for class_name in df['class'].unique(): # for each unique class name...\n",
        "\n",
        "    # ...print out the number of rows associated with it\n",
        "    print(class_name, \": \", sum(df['class'] == class_name), sep=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "id": "8dde4097",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>class</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Iris-setosa</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Iris-virginica</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 sepal_length  sepal_width  petal_length  petal_width\n",
              "class                                                                \n",
              "Iris-setosa                50           50            50           50\n",
              "Iris-versicolor            50           50            50           50\n",
              "Iris-virginica             50           50            50           50"
            ]
          },
          "execution_count": 262,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.groupby('class').count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc6a6e8-0398-4507-be5a-4aa9a76b4d90",
      "metadata": {
        "id": "7fc6a6e8-0398-4507-be5a-4aa9a76b4d90"
      },
      "source": [
        "In this case, our dataset isn't just relatively well balanced, it is exactly balanced, and each class has exactly 50 rows of data associated with it. However, if things were really skewed, for example, we had 100 rows of data for *Setosa*, 100 rows for *Versicolor*, and only 10 rows of data for *Virginica*, then we might need to find some way to make the data more balanced. Balancing datasets is way out of the scope of this tutorial, but if you'd like to learn more with this simple [Google search](https://www.google.com/search?q=how+to+balance+datasets)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4018d2c-7f27-40a1-ad71-b75b33f1ddbd",
      "metadata": {
        "id": "e4018d2c-7f27-40a1-ad71-b75b33f1ddbd"
      },
      "source": [
        "### Prepare the training data\n",
        "\n",
        "#### Split\n",
        "\n",
        "Now, let's split the data into **training** and **testing** datasets. The first step is to separate the columns into input values and labels.\n",
        "\n",
        "In this example, to keep the neural network simple, we'll just use `petal_width` and `sepal_width` values for the inputs. So the first we'll do is make sure we can correctly isolate the columns we want from the columns we don't want. We do this by passing `df` a list of column names we want to get values for, `['petal_width', 'sepal_width']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "id": "ebcdc455-78a9-4db7-88ed-cddf769b1ecb",
      "metadata": {
        "id": "ebcdc455-78a9-4db7-88ed-cddf769b1ecb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   petal_width  sepal_width\n",
              "0          0.2          3.5\n",
              "1          0.2          3.0\n",
              "2          0.2          3.2\n",
              "3          0.2          3.1\n",
              "4          0.2          3.6"
            ]
          },
          "execution_count": 263,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out the first few rows of just the `petal_width` and `sepal_width` columns\n",
        "df[['petal_width', 'sepal_width']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a37a2f-20a8-4f04-baa3-ae13cd9566dd",
      "metadata": {
        "id": "a2a37a2f-20a8-4f04-baa3-ae13cd9566dd"
      },
      "source": [
        "Now that we have confirmed that we can correctly isolate the values for `petal_width` and `sepal_width`, let's use the original DataFrame, `df`, to create two new DataFrames. One DataFrame will have the petal and sepal widths, the values we will use to make predictions, and we'll call this DataFrame `input_values`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "id": "63ae66da-a69f-4ac2-8f64-cad40b5c4daf",
      "metadata": {
        "id": "63ae66da-a69f-4ac2-8f64-cad40b5c4daf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.2</td>\n",
              "      <td>3.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   petal_width  sepal_width\n",
              "0          0.2          3.5\n",
              "1          0.2          3.0\n",
              "2          0.2          3.2\n",
              "3          0.2          3.1\n",
              "4          0.2          3.6"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_values = df[['petal_width', 'sepal_width']]\n",
        "input_values.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a8460f",
      "metadata": {},
      "source": [
        "The other DataFrame will have the species, the values we will use to determine how good those predictions are, and this DataFrame will be called `label_values`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "id": "43cee969-3620-4c84-b6e4-6a115229587a",
      "metadata": {
        "id": "43cee969-3620-4c84-b6e4-6a115229587a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    Iris-setosa\n",
            "1    Iris-setosa\n",
            "2    Iris-setosa\n",
            "3    Iris-setosa\n",
            "4    Iris-setosa\n",
            "Name: class, dtype: object\n"
          ]
        }
      ],
      "source": [
        "label_values = df['class']\n",
        "print(label_values.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1713310a-7341-49a1-bb7a-a1f5c989a459",
      "metadata": {
        "id": "1713310a-7341-49a1-bb7a-a1f5c989a459"
      },
      "source": [
        "Now, because neural networks expect the inputs and output values to be numbers, we need to convert the values in the `label_values` into numbers, and we'll do this with [`factorize()`](https://pandas.pydata.org/docs/reference/api/pandas.factorize.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "id": "3b692180",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object'))"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label_values.factorize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d7471c",
      "metadata": {},
      "source": [
        "It returns a list of lists (with codes and uniques), and since we only need the first list of values, we index the output of factorize() with `[0]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "id": "20c8f6af-e49a-4248-a024-020a77225677",
      "metadata": {
        "id": "20c8f6af-e49a-4248-a024-020a77225677"
      },
      "outputs": [],
      "source": [
        "# Convert the strings in the 'class' column into numbers with factorize()\n",
        "classes_as_numbers = label_values.factorize()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f666186a-93e1-42ab-b2b8-cad73602dd58",
      "metadata": {
        "id": "f666186a-93e1-42ab-b2b8-cad73602dd58"
      },
      "source": [
        "As we can see, the strings were converted into numbers. The first 50 values are 0, which represents *Setosa*. The following 50 values are 1, for *Versicolor*, and the last 50 values are 2, for *Viriginica*.\n",
        "\n",
        "Now, we need to split `input_values` and `classes_as_numbers` into **training** and **testing datasets**. And we do this with the **[sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)** function `train_test_split()`.\n",
        "\n",
        "_In practice, people usually use anywhere from 25-33% of the data for testing how well the model was trained. In this case, we'll use 25%, which is the default, but any percentage can be specified by setting the `test_size` parameter to a value between 0 and 1. Also, because we want to ensure that our test dataset has data for all three species of iris, we'll set `stratify=label_values`._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "id": "8de576eb-43fc-425a-8164-5f1f37d1dfc6",
      "metadata": {
        "id": "8de576eb-43fc-425a-8164-5f1f37d1dfc6"
      },
      "outputs": [],
      "source": [
        "input_train, input_test, label_train, label_test = train_test_split(\n",
        "    input_values,\n",
        "    classes_as_numbers,\n",
        "    test_size=0.25,\n",
        "    stratify=classes_as_numbers,\n",
        "    random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab8389e-1c0e-4c16-9990-745a7535b9cf",
      "metadata": {
        "id": "eab8389e-1c0e-4c16-9990-745a7535b9cf",
        "tags": []
      },
      "source": [
        "Now we can verify that `train_test_split()` correctly put 75% of the data into `input_train` and `input_test` by printing out their shapes. Remember 75% of 150 = 112.5, so we would expect both `input_train` and `input_test` to have 112 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "id": "41bdf25a-ebaa-4f20-99f3-21934064f532",
      "metadata": {
        "id": "41bdf25a-ebaa-4f20-99f3-21934064f532",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the training data set: (112, 2) \n",
            "Size of the training label set: (112,)\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Size of the training data set:\",\n",
        "    input_train.shape,\n",
        "    \"\\nSize of the training label set:\",\n",
        "    label_train.shape\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e518f978-bf7a-4842-a8d2-d473407992ce",
      "metadata": {
        "id": "e518f978-bf7a-4842-a8d2-d473407992ce"
      },
      "source": [
        "Both `input_train` and `label_train` have 112 rows, which is what we expect. Now, let's verify that the remaining 38 rows of data went into `input_test` and `label_test` by printing out their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "id": "bccd4630-49e7-4c3d-8111-ea3d89fa124d",
      "metadata": {
        "id": "bccd4630-49e7-4c3d-8111-ea3d89fa124d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the test data set: (38, 2) \n",
            "Size of the test label set: (38,)\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    \"Size of the test data set:\",\n",
        "    input_test.shape,\n",
        "    \"\\nSize of the test label set:\",\n",
        "    label_test.shape\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e069b36-9403-4bf7-a374-24151154f6ab",
      "metadata": {
        "id": "6e069b36-9403-4bf7-a374-24151154f6ab"
      },
      "source": [
        "#### One-hot encoding\n",
        "\n",
        "Now, because our neural network will have 3 outputs, one for each species (see the drawing of the neural network above), we need to convert the numbers in `label_train` into 3 element arrays, where each element in an array corresponds to a specific output in the neural network. Specifically, we'll use:\n",
        "\n",
        "- `[1.0, 0.0, 0.0]` to correspond to *Setosa*\n",
        "- `[0.0, 1.0, 0.0]` for *Versicolor*\n",
        "- `[0.0, 0.0, 1.0]` for *Virginica*\n",
        "\n",
        "The good news is that we can easily do the **[one-hot encoding](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)**. We also tack on `type(torch.float32)` to ensure the numbers are saved in the correct format for the neural network to process efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "id": "d1f679d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 2 1 1 1 2 0 2 0 2 0 2 1 0 0 1 2 0 0 1 1 1 0 1 2 0 2 1 2 0 0 1 0 2 0 0 1\n",
            " 0 1 0 0 1 2 2 0 2 1 0 2 0 2 2 0 1 2 2 1 1 0 1 1 2 1 2 0 1 0 2 1 2 1 2 2 0\n",
            " 2 1 0 2 0 2 1 1 0 2 2 0 0 2 2 1 2 0 2 1 2 2 0 1 1 1 1 1 0 2 1 1 0 0 0 0 1\n",
            " 0]\n"
          ]
        }
      ],
      "source": [
        "print(label_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "id": "49342d42-6b32-42a5-85a4-275339c6ac42",
      "metadata": {
        "id": "49342d42-6b32-42a5-85a4-275339c6ac42"
      },
      "outputs": [],
      "source": [
        "# Create a new tensor with one-hot encoded rows for each row in the original dataset.\n",
        "one_hot_label_train = F.one_hot(torch.tensor(label_train)).type(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883c5915-3b27-4c70-9c63-867d474be9fd",
      "metadata": {
        "id": "883c5915-3b27-4c70-9c63-867d474be9fd"
      },
      "source": [
        "If we printed out the entire contents of `one_hot_label_train`, we'd get a matrix with 150 rows, which would take up a lot of space. So, instead, let's print out the first 10 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "id": "b31ea02e-566b-4792-af27-cd6f0317402b",
      "metadata": {
        "id": "b31ea02e-566b-4792-af27-cd6f0317402b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., 0., 1.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print out a few of the rows one-hot encoded data.\n",
        "one_hot_label_train[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac937a1-5caa-4d76-9685-8a6d811c27b3",
      "metadata": {
        "id": "4ac937a1-5caa-4d76-9685-8a6d811c27b3"
      },
      "source": [
        "#### Normalization\n",
        "\n",
        "So, as we can see in the output above, `classes_as_numbers` was correctly one-hot encoded and saved in `one_hot_label_train`.\n",
        "\n",
        "Now, let's normalize the input variables so that their values range from 0 to 1. Normalizing data, so that it's all on the same scale, often makes it easier to train machine learning methods. In this case, since we have two datasets, `input_train` and `input_test`, we'll start determining the maximum and minimum values in `input_train`. Then we will use those values to normalize `input_train` and `input_test`. Using the maximum and minimum values from `input_train` to normalize both datasets avoids something called **Data Leakage**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "id": "d5185697-1816-44c1-80b5-2e7e2da7b00d",
      "metadata": {
        "id": "d5185697-1816-44c1-80b5-2e7e2da7b00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "petal_width    2.5\n",
            "sepal_width    4.4\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# First, determine the maximum values in input_train\n",
        "max_vals_in_input_train = input_train.max()\n",
        "\n",
        "print(max_vals_in_input_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "id": "ce5355db-7f14-48ae-a93e-7d66c2f4daa6",
      "metadata": {
        "id": "ce5355db-7f14-48ae-a93e-7d66c2f4daa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "petal_width    0.1\n",
            "sepal_width    2.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Second, determine the minimum values in input_train\n",
        "min_vals_in_input_train = input_train.min()\n",
        "\n",
        "print(min_vals_in_input_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "id": "33274229",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.192857</td>\n",
              "      <td>3.061607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.771533</td>\n",
              "      <td>0.440770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.100000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.300000</td>\n",
              "      <td>2.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.300000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.825000</td>\n",
              "      <td>3.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.500000</td>\n",
              "      <td>4.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       petal_width  sepal_width\n",
              "count   112.000000   112.000000\n",
              "mean      1.192857     3.061607\n",
              "std       0.771533     0.440770\n",
              "min       0.100000     2.000000\n",
              "25%       0.300000     2.800000\n",
              "50%       1.300000     3.000000\n",
              "75%       1.825000     3.300000\n",
              "max       2.500000     4.400000"
            ]
          },
          "execution_count": 276,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "id": "f8628abe-9ebe-4ccf-80cd-10e697d94b46",
      "metadata": {
        "id": "f8628abe-9ebe-4ccf-80cd-10e697d94b46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>112.000000</td>\n",
              "      <td>112.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.455357</td>\n",
              "      <td>0.442336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.321472</td>\n",
              "      <td>0.183654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.541667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       petal_width  sepal_width\n",
              "count   112.000000   112.000000\n",
              "mean      0.455357     0.442336\n",
              "std       0.321472     0.183654\n",
              "min       0.000000     0.000000\n",
              "25%       0.083333     0.333333\n",
              "50%       0.500000     0.416667\n",
              "75%       0.718750     0.541667\n",
              "max       1.000000     1.000000"
            ]
          },
          "execution_count": 277,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now normalize input_train with the maximum and minimum values from input_train\n",
        "input_train = (\n",
        "    (input_train - min_vals_in_input_train)\n",
        "    /\n",
        "    (max_vals_in_input_train - min_vals_in_input_train)\n",
        ")\n",
        "input_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "id": "c9c49642-aef9-402b-9473-6bc07a712fbb",
      "metadata": {
        "id": "c9c49642-aef9-402b-9473-6bc07a712fbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.464912</td>\n",
              "      <td>0.429825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.311583</td>\n",
              "      <td>0.173591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.343750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.708333</td>\n",
              "      <td>0.541667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.958333</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       petal_width  sepal_width\n",
              "count    38.000000    38.000000\n",
              "mean      0.464912     0.429825\n",
              "std       0.311583     0.173591\n",
              "min       0.041667     0.125000\n",
              "25%       0.083333     0.343750\n",
              "50%       0.541667     0.416667\n",
              "75%       0.708333     0.541667\n",
              "max       0.958333     0.833333"
            ]
          },
          "execution_count": 278,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now normalize input_test with the maximum and minimum values from input_train\n",
        "input_test = (input_test - min_vals_in_input_train) / (max_vals_in_input_train - min_vals_in_input_train)\n",
        "input_test.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0276bd3",
      "metadata": {},
      "source": [
        "We may see values more extreme, i.e., lower than 0 and higher than 1, in the test set. This is the expected behavior. The goal of normalization is not to force every single data point into the $[0, 1]$ range. The goal is to apply a consistent scaling transformation based on the knowledge gained from the training data alone. Values outside the $[0, 1]$ range in the test set are a normal and informative result of doing this correctly.\n",
        "\n",
        "The test set is supposed to be a completely unseen, pristine dataset that simulates how your model will perform on new, real-world data. If your model's training process has been influenced by any information from the test set—even something as simple as its minimum or maximum value—then the test set is no longer truly \"unseen.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383057be-98c7-4741-b3cd-20aa54a94d9a",
      "metadata": {
        "id": "383057be-98c7-4741-b3cd-20aa54a94d9a"
      },
      "source": [
        "#### DataLoader\n",
        "\n",
        "Now, let's put our training data into a **DataLoader**, which we can use to train the neural network. The DataLoader is a PyTorch utility that takes our final, fully prepared data and makes it easy to iterate over. It handles:\n",
        "\n",
        "- Batching: giving the model, say, 64 samples at a time instead of the whole dataset.\n",
        "- Shuffling: randomizing the order of the data each epoch to prevent the model from learning the sequence.\n",
        "- Parallelism: using multiple CPU cores to load data in the background so the GPU doesn't have to wait.\n",
        "\n",
        "DataLoaders are great for large datasets because they make it easy to access the data in batches, make it easy to shuffle the data each epoch, and they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging our code.\n",
        "\n",
        "To put our data training data into a DataLoader, we'll start by converting `input_train` into **tensors** with `torch.tensor()`. We'll then combine `input_train` with `one_hot_label_train` to create a **TensorDataset**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "id": "96216cf7-498e-4498-beb7-9e1b50a7bf84",
      "metadata": {
        "id": "96216cf7-498e-4498-beb7-9e1b50a7bf84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7500, 0.3333],\n",
              "        [0.7917, 0.3333],\n",
              "        [0.3750, 0.1667],\n",
              "        [0.5000, 0.3333],\n",
              "        [0.5000, 0.2083]])"
            ]
          },
          "execution_count": 279,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the DataFrame input_train into tensors\n",
        "input_train_tensors = torch.tensor(input_train.values).type(torch.float32)\n",
        "\n",
        "# now print out the first 5 rows to make sure they are what we expect.\n",
        "input_train_tensors[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de6d9df-347e-4017-bed7-80244e8c52fa",
      "metadata": {
        "id": "4de6d9df-347e-4017-bed7-80244e8c52fa"
      },
      "source": [
        "Because we'll also need to run `input_test` through the neural network, we'll need to convert it to tensors as well, and we might as well do it now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "id": "43d15911-d4b0-4ccc-88ea-055a2be920a5",
      "metadata": {
        "id": "43d15911-d4b0-4ccc-88ea-055a2be920a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.0417, 0.5000],\n",
              "        [0.6250, 0.5417],\n",
              "        [0.5000, 0.3333],\n",
              "        [0.5000, 0.1250],\n",
              "        [0.0417, 0.4167]])"
            ]
          },
          "execution_count": 280,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Convert the DataFrame input_test into tensors\n",
        "input_test_tensors = torch.tensor(input_test.values).type(torch.float32)\n",
        "\n",
        "## now print out the first 5 rows to make sure they are what we expect.\n",
        "input_test_tensors[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb8dd94d-c620-415c-8d06-3267e415c1d9",
      "metadata": {
        "id": "cb8dd94d-c620-415c-8d06-3267e415c1d9"
      },
      "source": [
        "Now that we have tensors for `input_train`, named `input_train_tensors`, and we have the one-hot encoded `class` values stored in tensors called `label_train`, we can combine them into a **TensorDataset** that are, in turn, turned into **DataLoader**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "id": "46a23b9c-ae55-41cd-9828-db0f0eb39e3d",
      "metadata": {
        "id": "46a23b9c-ae55-41cd-9828-db0f0eb39e3d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(input_train_tensors, one_hot_label_train)\n",
        "train_dataloader = DataLoader(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31955e37-156e-4c5b-b4bd-806d0e6e2d74",
      "metadata": {
        "id": "31955e37-156e-4c5b-b4bd-806d0e6e2d74"
      },
      "source": [
        "### Refactoring\n",
        "\n",
        "Now that we saw how to execute all these steps, let's write a more robust and reusable pipeline, using scaler and label encoder tools from `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "id": "08a59b86",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Import the scaler and label encoder from sklearn ---\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- Load and Define X and y ---\n",
        "path = \"./iris.txt\"\n",
        "# Assign column names since the file has no header\n",
        "column_names = [\n",
        "    'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "\n",
        "df = pd.read_csv(\n",
        "    path, sep=\",\", names=column_names)\n",
        "\n",
        "# --- Define X and y ---\n",
        "input_values = df[['petal_width', 'sepal_width']]\n",
        "label_values = df['class']\n",
        "\n",
        "# --- SPLIT THE DATA FIRST ---\n",
        "# (This is best practice)\n",
        "input_train, input_test, label_train_str, label_test_str = train_test_split(\n",
        "    input_values,\n",
        "    label_values,           # split the original label strings\n",
        "    test_size=0.25,\n",
        "    stratify=label_values,  # stratify on the label strings\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- SKLEARN PREPROCESSING ---\n",
        "# Initialize and fit the LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit(label_train_str)  # Fit *only* on the training labels\n",
        "\n",
        "# Transform *both* sets\n",
        "label_train_num = le.transform(label_train_str)\n",
        "label_test_num = le.transform(label_test_str)\n",
        "\n",
        "# (We can also use le.fit_transform(label_train_str) for the first one)\n",
        "\n",
        "# Initialize and fit the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(input_train)\n",
        "input_train_scaled = scaler.transform(input_train)\n",
        "input_test_scaled = scaler.transform(input_test)\n",
        "\n",
        "# --- PYTORCH CONVERSION ---\n",
        "# Convert scaled data to tensors\n",
        "input_train_tensors = torch.tensor(input_train_scaled, dtype=torch.float32)\n",
        "input_test_tensors = torch.tensor(input_test_scaled, dtype=torch.float32)\n",
        "\n",
        "# Convert encoded labels to tensors\n",
        "label_train_tensors = torch.tensor(label_train_num)\n",
        "label_test_tensors = torch.tensor(label_test_num)\n",
        "\n",
        "# One-hot encode the numeric labels\n",
        "# The model's output layer will produce float32 predictions. \n",
        "# To calculate the loss, PyTorch requires the predictions \n",
        "# and the labels to be the same data type.\n",
        "one_hot_label_train = F.one_hot(label_train_tensors).type(torch.float32)\n",
        "one_hot_label_test = F.one_hot(label_test_tensors).type(torch.float32)\n",
        "\n",
        "# Create TensorDataset and DataLoader as before\n",
        "train_dataset = TensorDataset(input_train_tensors, one_hot_label_train)\n",
        "train_dataloader = DataLoader(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea",
      "metadata": {
        "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea"
      },
      "source": [
        "## Build a neural network\n",
        "\n",
        "Building a neural network with PyTorch means creating a new class. And to make it easy to train the neural network, this class will inherit from `LightningModule`.\n",
        "\n",
        "Our new class will have the following methods:\n",
        "\n",
        "- `__init__()` to initialize the Weights and Biases and keep track of a few other housekeeping things.\n",
        "- `forward()` to make a forward pass through the neural network.\n",
        "- `configure_optimizers()` to configure the optimizer. There are lots of optimizers to choose from, but in this tutorial, we'll change things up and use `Adam`.\n",
        "- `training_step()` to pass the training data to `forward()`, calculate the loss and keep track of the loss values in a log file.\n",
        "\n",
        "Also, for reference, here is a picture of the neural network we want to create:\n",
        "\n",
        "<img src=\"./images/final_nn.png\" alt=\"a neural network with multiple inputs and outputs\" style=\"width: 1000px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f26851-96df-4a84-b1ce-3351e61adf25",
      "metadata": {
        "id": "73f26851-96df-4a84-b1ce-3351e61adf25"
      },
      "source": [
        "As we can see, our neural network has **2 inputs**, one for `Petal Width` and one for `Sepal Width`, a single _hidden layer_ with two **ReLU** activation functions, and **3 outputs**, one for each species of iris.\n",
        "\n",
        "So, given this specification for this neural network, let's code it in a new class called `MultipleInsOuts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "id": "611711c1-ea20-48fa-bb89-c2ce5074dde7",
      "metadata": {
        "id": "611711c1-ea20-48fa-bb89-c2ce5074dde7"
      },
      "outputs": [],
      "source": [
        "class MultipleInsOuts(L.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Set the seed for the random number generator.\n",
        "        L.seed_everything(seed=42)\n",
        "        \n",
        "        # When self.linear_layer = nn.Linear() is executed, PyTorch automatically \n",
        "        # initializes the weight and bias tensors for this layer with random values \n",
        "        # drawn from a specific distribution (see below).\n",
        "\n",
        "        ## We don't have to specifiy each and every single Weight and Bias values!\n",
        "\n",
        "        ############################################################################\n",
        "        ##\n",
        "        ## Here is where we initialize the Weights and Biases for the neural network\n",
        "        ##\n",
        "        ############################################################################\n",
        "\n",
        "        # If we look at the drawing of the network we want to build (above),\n",
        "        # we see that we have 2 inputs that lead to 2 activation functions.\n",
        "        # We create these connections and **initialize their Weights and Biases**\n",
        "        # with the nn.Linear() function by setting in_features=2 and out_features=2\n",
        "        self.input_to_hidden = nn.Linear(in_features=2, out_features=2, bias=True)\n",
        "\n",
        "        # Next, we see that the 2 activation functions are connected to 3 outputs.\n",
        "        # We create these connections and initialize their Weights and Biases\n",
        "        # with the nn.Linear() function by setting in_features=2 and out_features=3.\n",
        "        self.hidden_to_output = nn.Linear(in_features=2, out_features=3, bias=True)\n",
        "\n",
        "        self.loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        # First, we run the input values to the activation functions \n",
        "        # in the hidden layer\n",
        "        hidden = self.input_to_hidden(input)\n",
        "\n",
        "        # Then we run the values through a ReLU activation function \n",
        "        # and then run those values to the output\n",
        "        output_values = self.hidden_to_output(torch.relu(hidden))\n",
        "\n",
        "        return(output_values)\n",
        "    \n",
        "        # We could also have defined the entire net in __init__() such as \n",
        "        # self.net = nn.Sequential(\n",
        "        #     nn.Linear(in_features=2, out_features=2),  # Linear\n",
        "        #     nn.ReLU(),                                 # ReLU\n",
        "        #     nn.Linear(in_features=2, out_features=3),  # Linear\n",
        "        # )\n",
        "        # and then in forward() method:\n",
        "        # return self.net(input)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # In this example, configuring the optimizer\n",
        "        # consists of passing it the weights and biases we want\n",
        "        # to optimize, which are all in self.parameters(),\n",
        "        # and setting the learning rate with lr=0.001.\n",
        "\n",
        "        # Adam (Adaptive Moment Estimation) is a little less stochastic \n",
        "        # than SGD by using weighted average beteween each step\n",
        "        # https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam\n",
        "\n",
        "        return Adam(self.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # The first thing we do is split 'batch' into \n",
        "        # the input and label values\n",
        "        inputs, labels = batch\n",
        "\n",
        "        # Then we run the input through the neural network\n",
        "        outputs = self.forward(inputs)\n",
        "\n",
        "        # Then we calculate the loss.\n",
        "        loss = self.loss(outputs, labels)\n",
        "\n",
        "        # Lastly, we could add the loss a log file so that \n",
        "        # we can graph it later. This would help us decide \n",
        "        # if we have done enough training. Ideally, if we\n",
        "        # do enough training, the loss should be small \n",
        "        # and not getting any smaller.\n",
        "        #self.log(\"loss\", loss)\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "id": "9b76f813-a366-4f23-ac77-5f48e62cd8cc",
      "metadata": {
        "id": "9b76f813-a366-4f23-ac77-5f48e62cd8cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Seed set to 42\n"
          ]
        }
      ],
      "source": [
        "model = MultipleInsOuts()  # First, make model from the class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "id": "4e0e2da6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_to_hidden.weight \n",
            " tensor([[ 0.5406,  0.5869],\n",
            "        [-0.1657,  0.6496]])\n",
            "input_to_hidden.bias \n",
            " tensor([-0.1549,  0.1427])\n",
            "hidden_to_output.weight \n",
            " tensor([[-0.3443,  0.4153],\n",
            "        [ 0.6233, -0.5188],\n",
            "        [ 0.6146,  0.1323]])\n",
            "hidden_to_output.bias \n",
            " tensor([0.5224, 0.0958, 0.3410])\n"
          ]
        }
      ],
      "source": [
        "# Now print out the name and value for each named parameter\n",
        "# in the model, like Weights and Biases, that we can train.\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, '\\n', param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "id": "a49fa396",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('input_to_hidden.weight',\n",
              "              tensor([[ 0.5406,  0.5869],\n",
              "                      [-0.1657,  0.6496]])),\n",
              "             ('input_to_hidden.bias', tensor([-0.1549,  0.1427])),\n",
              "             ('hidden_to_output.weight',\n",
              "              tensor([[-0.3443,  0.4153],\n",
              "                      [ 0.6233, -0.5188],\n",
              "                      [ 0.6146,  0.1323]])),\n",
              "             ('hidden_to_output.bias', tensor([0.5224, 0.0958, 0.3410]))])"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Or we can print the parameters directly as a dictionary\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4af095",
      "metadata": {},
      "source": [
        "The `nn.Linear` weight matrix is structured as `[out_features, in_features]`. Therefore, the first row fo the 'input_to_hidden.weight' tensor contains the weights leading to H₀, and the second row contains the weights leading to H₁. This correspond to the following state of the neural network:\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Style Definitions\n",
        "    classDef biasStyle fill:none,stroke:none,color:#7f8c8d;\n",
        "\n",
        "    %% Input Layer (2 nodes)\n",
        "    subgraph Input Layer [Inputs]\n",
        "        direction TB\n",
        "        I0((\"I₀\"))\n",
        "        I1((\"I₁\"))\n",
        "    end\n",
        "\n",
        "    %% Hidden Layer (2 nodes)\n",
        "    subgraph Hidden Layer [Hidden]\n",
        "        direction TB\n",
        "        H0((\"H₀\"))\n",
        "        H1((\"H₁\"))\n",
        "    end\n",
        "\n",
        "    %% Output Layer (3 nodes)\n",
        "    subgraph Output Layer [Outputs]\n",
        "        direction TB\n",
        "        O0((\"O₀\"))\n",
        "        O1((\"O₁\"))\n",
        "        O2((\"O₂\"))\n",
        "    end\n",
        "\n",
        "    %% Bias Nodes for Hidden Layer\n",
        "    BH0[\"Bias: -0.15\"]:::biasStyle\n",
        "    BH1[\"Bias: 0.14\"]:::biasStyle\n",
        "\n",
        "    %% Bias Nodes for Output Layer\n",
        "    BO0[\"Bias: 0.52\"]:::biasStyle\n",
        "    BO1[\"Bias: 0.10\"]:::biasStyle\n",
        "    BO2[\"Bias: 0.34\"]:::biasStyle\n",
        "\n",
        "    %% Connections from Input to Hidden Layer Weights\n",
        "    I0 -->|\"0.54\"| H0\n",
        "    I1 -->|\"0.59\"| H0\n",
        "    \n",
        "    I0 -->|\"-0.17\"| H1\n",
        "    I1 -->|\"0.65\"| H1\n",
        "\n",
        "    %% Connections from Hidden Layer Biases\n",
        "    BH0 -.-> H0\n",
        "    BH1 -.-> H1\n",
        "\n",
        "    %% Connections from Hidden to Output Layer Weights\n",
        "    H0 -->|\"-0.34\"| O0\n",
        "    H1 -->|\"0.42\"| O0\n",
        "    \n",
        "    H0 -->|\"0.62\"| O1\n",
        "    H1 -->|\"-0.52\"| O1\n",
        "    \n",
        "    H0 -->|\"0.61\"| O2\n",
        "    H1 -->|\"0.13\"| O2\n",
        "\n",
        "    %% Connections from Output Layer Biases\n",
        "    BO0 -.-> O0\n",
        "    BO1 -.-> O1\n",
        "    BO2 -.-> O2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "16b3d5c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: MultipleInsOuts(\n",
            "  (input_to_hidden): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (hidden_to_output): Linear(in_features=2, out_features=3, bias=True)\n",
            "  (loss): MSELoss()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Model:\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 293,
      "id": "e38e423c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from the initialized NN:\n",
            "tensor([1., 0., 1.], grad_fn=<RoundBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Run different a set of values through the current (not optimized)\n",
        "# neural network through `forward`. Note that the model learns to \n",
        "# expect inputs in this [0, 1] range, so that the value in the input\n",
        "# tensor represent the scaled values for Petal and Sepal Widths.\n",
        "output_values = model(torch.tensor([0.5, .35]))\n",
        "\n",
        "print(\"Output from the initialized NN:\")\n",
        "print(torch.round(output_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645699fb-e215-46f2-b730-2e9276d0a407",
      "metadata": {
        "id": "645699fb-e215-46f2-b730-2e9276d0a407",
        "tags": []
      },
      "source": [
        "## Train the Neural Network\n",
        "\n",
        "Now that we've created a class for our neural network, let's train it.\n",
        "\n",
        "Training our new neural network means we create a **Lightning Trainer**, `L.Trainer`, and use it to optimize the parameters.\n",
        "\n",
        "We will start with 10 epochs, complete runs through our training data. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out later in the tutorial when we make a graph of how the loss values change during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d",
      "metadata": {
        "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name             | Type    | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | input_to_hidden  | Linear  | 6      | train\n",
            "1 | hidden_to_output | Linear  | 9      | train\n",
            "2 | loss             | MSELoss | 0      | train\n",
            "-----------------------------------------------------\n",
            "15        Trainable params\n",
            "0         Non-trainable params\n",
            "15        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "3         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "c:\\Users\\Sébastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89b8ac2e329c484b80116a0b1210b820",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(max_epochs=10)\n",
        "trainer.fit(model, train_dataloaders=train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "id": "b0b029e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('input_to_hidden.weight',\n",
              "              tensor([[ 0.9860,  0.3223],\n",
              "                      [-0.3675,  0.7899]])),\n",
              "             ('input_to_hidden.bias', tensor([-0.1776,  0.0590])),\n",
              "             ('hidden_to_output.weight',\n",
              "              tensor([[-0.7209,  0.7427],\n",
              "                      [ 0.4759, -0.4373],\n",
              "                      [ 0.7126, -0.2330]])),\n",
              "             ('hidden_to_output.bias', tensor([0.4505, 0.2084, 0.1178]))])"
            ]
          },
          "execution_count": 295,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f",
      "metadata": {
        "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f"
      },
      "source": [
        "We've trained the model with 10 epochs! Now, let's see if the predictions are any good. We can do this by seeing how well it predicts the testing data. We'll start by running `input_test_tensors` through the neural network and saving the output `predictions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "id": "0fcdcfd9-36f1-44d2-ae8c-3b3f78e99693",
      "metadata": {
        "id": "0fcdcfd9-36f1-44d2-ae8c-3b3f78e99693"
      },
      "outputs": [],
      "source": [
        "# Run the input_test_tensors through the neural network\n",
        "predictions = model(input_test_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d258a469-33ae-4941-a5b8-86f9b1499ac4",
      "metadata": {
        "id": "d258a469-33ae-4941-a5b8-86f9b1499ac4"
      },
      "source": [
        "Now, because our neural network has three outputs, one for each Iris species, we should get 3 values for each row in `input_test_tensors`. We can verify that by looking at the first few rows of `predictions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "id": "6ec2ed77-e863-40fb-9142-2da3ec9f3f78",
      "metadata": {
        "id": "6ec2ed77-e863-40fb-9142-2da3ec9f3f78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7585, 0.0282, 0.0331],\n",
              "        [0.1994, 0.3877, 0.4948],\n",
              "        [0.2486, 0.3490, 0.3868],\n",
              "        [0.1941, 0.3776, 0.3712],\n",
              "        [0.7274, 0.0453, 0.0309]], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": 297,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions[:5,]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52512e0f-9a49-4080-aeea-ceac0745ca5a",
      "metadata": {
        "id": "52512e0f-9a49-4080-aeea-ceac0745ca5a"
      },
      "source": [
        "We can determine which species was predicted in `predictions` by selecting the index in each row that corresponding to the largest value, and we do that with `torch.argmax()`. It returns a tensor that contains the indices with the largest values for each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "id": "8a336e70-1e01-48a2-9761-cb51c845dbbb",
      "metadata": {
        "id": "8a336e70-1e01-48a2-9761-cb51c845dbbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 2, 2, 1, 0])"
            ]
          },
          "execution_count": 298,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Select the output with highest value\n",
        "predicted_labels = torch.argmax(predictions, dim=1) # dim=1 applies argmax to columns\n",
        "predicted_labels[0:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb622159-b415-4e2e-83af-328cabad438e",
      "metadata": {
        "id": "bb622159-b415-4e2e-83af-328cabad438e"
      },
      "source": [
        "In the first and last rows index 0 had the largest value. Thus, these prediction corresponds to *Setosa*. The second and third predicted 2, which corresponds to *Virginica*. The fourth predicted *Versicolor*.\n",
        "\n",
        "Now, let's compare what the neural network predicted in `predicted_labels` to the known values in `label_test` and calculate the percentage of correct predictions. We do this by adding up the number of times an element in `predicted_labels` equals the corresponding element in `label_test` and dividing by the number of elements in `predicted_labels`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "id": "0872e381-57ef-43e3-a6b4-9aa9d96c93e4",
      "metadata": {
        "id": "0872e381-57ef-43e3-a6b4-9aa9d96c93e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.7368)"
            ]
          },
          "execution_count": 299,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now compare predicted_labels with test_labels to calculate accuracy.\n",
        "# `torch.eq()` computes element-wise equality between two tensors.\n",
        "# label_test, however, is just an array, so we convert it to a tensor\n",
        "# before passing it in. `torch.sum()` then adds up all of the \"True\"\n",
        "# output values to get the number of correct predictions.\n",
        "# We then divide the number of correct predictions by the number of predicted values,\n",
        "# obtained with len(predicted_labels), to get the percentage of correct predictions\n",
        "\n",
        "torch.sum(torch.eq(torch.tensor(label_test), predicted_labels)) / len(predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261fdd41-2caa-43be-8c78-0c2d64f08f3d",
      "metadata": {
        "id": "261fdd41-2caa-43be-8c78-0c2d64f08f3d"
      },
      "source": [
        "And we see that our neural network only correctly predicts 73.7% of the testing data. This isn't very good. So, will training our model for more epochs improve the model's predictions?\n",
        "\n",
        "One way to answer that question is to just train for longer and see what happens.\n",
        "\n",
        "The good news is that because we're using **Lightning**, we can pick up where we left off training without starting over from scratch. This is because training with **Lightning** creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are. This is awesome and will save us a lot of time since we don't have to retrain the first 10 epochs. So, let's add an additional 90 epochs to the training.\n",
        "\n",
        "To add additional epochs to the training, we first identify where the checkpoint file is with the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 300,
      "id": "29bc4c1b-911d-417f-8ab2-9f52712502e8",
      "metadata": {
        "id": "29bc4c1b-911d-417f-8ab2-9f52712502e8"
      },
      "outputs": [],
      "source": [
        "path_to_checkpoint = trainer.checkpoint_callback.best_model_path  # By default, \"best\" = \"most recent\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d4fd497-997c-4369-9b3e-d25cd0911d5d",
      "metadata": {
        "id": "6d4fd497-997c-4369-9b3e-d25cd0911d5d"
      },
      "source": [
        "By doing so, we ask for the manual and intentional process of resuming training. However, if we restart the Jupyter kernel, the Trainer is smart and looks in its default save directory (e.g., \"./checkpoints/\" or \"./lightning_logs/\") and sees the model we trained last time. So it's good to remove old checkpoint folder if we don't need them anymore.\n",
        "\n",
        "Let's create a new Lightning Trainer, just like before, but we set the number of epochs to 100. Given that we already trained for 10 epochs, this means we'll do 90 more. We can observe that the training process actually starts at epoch number 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "id": "5a849d2f-bad1-4214-a72b-f1b04174639c",
      "metadata": {
        "id": "5a849d2f-bad1-4214-a72b-f1b04174639c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Restoring states from the checkpoint path at c:\\Users\\Sébastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_03\\lightning_logs\\version_0\\checkpoints\\epoch=9-step=1120.ckpt\n",
            "c:\\Users\\Sébastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:445: The dirpath has changed from 'c:\\\\Users\\\\Sébastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_03\\\\lightning_logs\\\\version_0\\\\checkpoints' to 'c:\\\\Users\\\\Sébastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_03\\\\lightning_logs\\\\version_1\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
            "\n",
            "  | Name             | Type    | Params | Mode \n",
            "-----------------------------------------------------\n",
            "0 | input_to_hidden  | Linear  | 6      | train\n",
            "1 | hidden_to_output | Linear  | 9      | train\n",
            "2 | loss             | MSELoss | 0      | train\n",
            "-----------------------------------------------------\n",
            "15        Trainable params\n",
            "0         Non-trainable params\n",
            "15        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "3         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Restored all states from the checkpoint at c:\\Users\\Sébastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_03\\lightning_logs\\version_0\\checkpoints\\epoch=9-step=1120.ckpt\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b5f4226882a4eb6b0c8c4b809fe6800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
          ]
        }
      ],
      "source": [
        "# First, create a new Lightning Trainer\n",
        "trainer = L.Trainer(max_epochs=100)  # Before, max_epochs=10, so, by setting it to 100, we're adding 90 more.\n",
        "\n",
        "# Then call trainer.fit() using the path to the most recent checkpoint files\n",
        "# so that we can pick up where we left off.\n",
        "trainer.fit(model, train_dataloaders=train_dataloader, ckpt_path=path_to_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e",
      "metadata": {
        "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e"
      },
      "source": [
        "Now, let's run the testing data through the network and calculate the accuracy. We'll do this just like we did before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "id": "930f26fe-eca7-46a3-9652-0e1d1b1bcfb6",
      "metadata": {
        "id": "930f26fe-eca7-46a3-9652-0e1d1b1bcfb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.9211)"
            ]
          },
          "execution_count": 304,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the input_test_tensors through the neural network\n",
        "predictions = model(input_test_tensors)\n",
        "\n",
        "# Select the output with highest value...\n",
        "predicted_labels = torch.argmax(predictions, dim=1) ## dim=0 applies softmax to rows, dim=1 applies softmax to columns\n",
        "\n",
        "# Now compare predicted_labels with test_labels to calculate accuracy\n",
        "torch.sum(torch.eq(torch.tensor(label_test), predicted_labels)) / len(predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31918cf1-5cfc-475b-b61a-f08525111741",
      "metadata": {
        "id": "31918cf1-5cfc-475b-b61a-f08525111741"
      },
      "source": [
        "After 100 training epochs, we correctly classified 92.1% of the testing data. This means adding more training was helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "b9e495aa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "OrderedDict([('input_to_hidden.weight',\n",
              "              tensor([[ 1.4299,  0.0198],\n",
              "                      [-1.1063,  0.8932]])),\n",
              "             ('input_to_hidden.bias', tensor([-0.4824,  0.1904])),\n",
              "             ('hidden_to_output.weight',\n",
              "              tensor([[-0.2433,  1.2659],\n",
              "                      [-0.7735, -1.1830],\n",
              "                      [ 1.4803,  0.2863]])),\n",
              "             ('hidden_to_output.bias', tensor([ 0.1418,  0.8101, -0.1898]))])"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943105dd",
      "metadata": {},
      "source": [
        "This corresponds to the following NN configuration.\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Style Definitions\n",
        "    classDef biasStyle fill:none,stroke:none,color:#7f8c8d;\n",
        "\n",
        "    %% Input Layer (2 nodes)\n",
        "    subgraph Input Layer [Inputs]\n",
        "        direction TB\n",
        "        I0((\"I₀\"))\n",
        "        I1((\"I₁\"))\n",
        "    end\n",
        "\n",
        "    %% Hidden Layer (2 nodes)\n",
        "    subgraph Hidden Layer [\"Hidden (ReLU)\"]\n",
        "        direction TB\n",
        "        H0((\"H₀\"))\n",
        "        H1((\"H₁\"))\n",
        "    end\n",
        "\n",
        "    %% Output Layer (3 nodes)\n",
        "    subgraph Output Layer [Outputs]\n",
        "        direction TB\n",
        "        O0((\"O₀\"))\n",
        "        O1((\"O₁\"))\n",
        "        O2((\"O₂\"))\n",
        "    end\n",
        "\n",
        "    %% Bias Nodes for Hidden Layer\n",
        "    BH0[\"Bias: -0.48\"]:::biasStyle\n",
        "    BH1[\"Bias: 0.19\"]:::biasStyle\n",
        "\n",
        "    %% Bias Nodes for Output Layer\n",
        "    BO0[\"Bias: 0.15\"]:::biasStyle\n",
        "    BO1[\"Bias: 0.81\"]:::biasStyle\n",
        "    BO2[\"Bias: -0.19\"]:::biasStyle\n",
        "\n",
        "    %% Connections from Input to Hidden Layer Weights\n",
        "    I0 -->|\"1.43\"| H0\n",
        "    I1 -->|\"0.02\"| H0\n",
        "    \n",
        "    I0 -->|\"-1.11\"| H1\n",
        "    I1 -->|\"0.89\"| H1\n",
        "\n",
        "    %% Connections from Hidden Layer Biases\n",
        "    BH0 -.-> H0\n",
        "    BH1 -.-> H1\n",
        "\n",
        "    %% Connections from Hidden to Output Layer Weights\n",
        "    H0 -->|\"-0.24\"| O0\n",
        "    H1 -->|\"1.27\"| O0\n",
        "    \n",
        "    H0 -->|\"-0.77\"| O1\n",
        "    H1 -->|\"-1.18\"| O1\n",
        "    \n",
        "    H0 -->|\"1.48\"| O2\n",
        "    H1 -->|\"0.29\"| O2\n",
        "\n",
        "    %% Connections from Output Layer Biases\n",
        "    BO0 -.-> O0\n",
        "    BO1 -.-> O1\n",
        "    BO2 -.-> O2\n",
        "```\n",
        "\n",
        "If we now pass the input tensor that represents the scaled values for Petal and Sepal Widths used in the book to the trained model, we receive the output *Versicolor*, as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "id": "c8b2b57c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0834, 0.6245, 0.1653], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 305,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(torch.tensor([0.5, .37]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "5d746770",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6233520000000001"
            ]
          },
          "execution_count": 229,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Manual calculation to get the output of the second cell\n",
        "(\n",
        "    (  # H0\n",
        "        max(0, \n",
        "            (.5 * 1.43)\n",
        "            +\n",
        "            (.37 * .02)\n",
        "            +\n",
        "            -.48\n",
        "        ) * -.77\n",
        "    )\n",
        "    +\n",
        "    (  # H1\n",
        "        max(0, \n",
        "            (.5 * -1.11)\n",
        "            +\n",
        "            (.37 * .89)\n",
        "            +\n",
        "            .19\n",
        "        ) * -1.18\n",
        "    )\n",
        "    +\n",
        "    .81\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098",
      "metadata": {
        "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098"
      },
      "source": [
        "## Make prediction\n",
        "\n",
        "Now that our model is trained, we can use it to make predictions from new data. This is done by passing the model a tensor with normalized petal and sepal widths wrapped up in a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "id": "3ab03556",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>petal_width</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>1.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.3</td>\n",
              "      <td>3.8</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>2.3</td>\n",
              "      <td>2.6</td>\n",
              "      <td>Iris-virginica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>1.5</td>\n",
              "      <td>2.9</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1.4</td>\n",
              "      <td>2.8</td>\n",
              "      <td>Iris-versicolor</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     petal_width  sepal_width            class\n",
              "73           1.2          2.8  Iris-versicolor\n",
              "18           0.3          3.8      Iris-setosa\n",
              "118          2.3          2.6   Iris-virginica\n",
              "78           1.5          2.9  Iris-versicolor\n",
              "76           1.4          2.8  Iris-versicolor"
            ]
          },
          "execution_count": 313,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[['petal_width', 'sepal_width', 'class']].sample(n=5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8e416a",
      "metadata": {},
      "source": [
        "For example, if the raw Petal and Sepal width measurements were 0.3 and 3.8, like the values from the *Setosa* individual in row 18, we would first normalize them using the maximum and minimum values we calculated with the training data.\n",
        "\n",
        "We have to think as the `MinMaxScaler` as **part of the model itself**, as it has learned the properties of the training data. Therefore for consistency, we must treat the new, unseen data exactly the same way we treated the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "id": "d0274c5e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([np.float64(0.3), np.float64(3.8)], dtype=object)"
            ]
          },
          "execution_count": 321,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[18, ['petal_width', 'sepal_width']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8759e954-3d0b-4ac3-84ac-5c294585643a",
      "metadata": {
        "id": "8759e954-3d0b-4ac3-84ac-5c294585643a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Petal and sepal widths =  [[0.3 3.8]]\n",
            "Normalized petal and sepal widths =  [[0.08333333 0.75      ]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sébastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#petal_sepal_widths = df.loc[18, ['petal_width', 'sepal_width']].values.reshape(1, -1)\n",
        "#normalized_values = (petal_sepal_widths - min_vals_in_input_train) / (max_vals_in_input_train - min_vals_in_input_train)\n",
        "petal_sepal_widths = np.array([.3, 3.8]).reshape(1, -1)\n",
        "print(\"Petal and sepal widths = \", petal_sepal_widths)\n",
        "\n",
        "normalized_petal_sepal_widths = scaler.transform(petal_sepal_widths)\n",
        "print(\"Normalized petal and sepal widths = \", normalized_petal_sepal_widths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "211e1e78-d0dd-46c8-9cd5-f59815385dea",
      "metadata": {
        "id": "211e1e78-d0dd-46c8-9cd5-f59815385dea"
      },
      "source": [
        "Then we convert `normalized_values` into a tensor and pass it to the model to see what it predicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2759e9fd-0f5e-40f2-a004-572fa2daf97b",
      "metadata": {
        "id": "2759e9fd-0f5e-40f2-a004-572fa2daf97b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.1142, -0.0986,  0.0302]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 312,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(torch.tensor(normalized_petal_sepal_widths, dtype=torch.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2053241f-37bb-44b3-b2da-38e841679422",
      "metadata": {
        "id": "2053241f-37bb-44b3-b2da-38e841679422"
      },
      "source": [
        "And first output has the largest value, meaning that the neural network predicts that the measurements come from *Setosa*, as expected."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
