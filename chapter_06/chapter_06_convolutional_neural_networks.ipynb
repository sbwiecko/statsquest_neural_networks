{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {},
   "source": [
    "# Chapter 6 -- Convolutional Neural Networks\n",
    "\n",
    "In this notebook, we will use **[PyTorch](https://pytorch.org/) and [Lightning](https://www.lightning.ai/)** to create and optimize a simple [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) like the one shown in the picture below. Convolutional neural networks are used for image classification, so when your phone thinks you've taken a picture of a cute kitten, that picture was probably classified with a CNN. In this case, we will create a network that can classify Xs and Os during a game of tic-tac-toe.\n",
    "\n",
    "<img src=\"./images/annotated_cnn.png\" alt=\"a simple convolutional neural network\" style=\"width: 800px;\">\n",
    "<!-- <img src=\"./images/full_cnn.png\" alt=\"a simple convolutional neural network\" style=\"width: 800px;\"> -->\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "- Create data and then build a DataLoader\n",
    "- Build a simple Convolutional Neural Network\n",
    "- Train the Convolutional Neural Network\n",
    "- Classify new images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4feccd-8472-4bb2-bc56-ea41e527d714",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "### Import PyTorch and Lightning\n",
    "\n",
    "The very first thing we need to do is load a bunch of Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"]= (2,2)  # Plot smaler images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23672f4-c30e-42de-9d35-5fa6b9bfd0f5",
   "metadata": {},
   "source": [
    "### Create Images and then Build a DataLoader From Scratch.\n",
    "\n",
    "We need to create the images to train and test our neural network. Specifically, we'll create an image of the letters **O** and **X** that look like the pictures below.\n",
    "\n",
    "<img src=\"./images/training_data.png\" alt=\"a convolutional neural network\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcc4d3-9dd9-42d8-87ab-aea0472448fc",
   "metadata": {},
   "source": [
    "We'll start by creating the image of the letter **O** by creating a 6x6 matrix of numbers where 0 represents white and 1 represents black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fc065c-c267-44ba-b62c-4e96f58b9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 6x6 matrix of numbers where 0 represents white\n",
    "# and 1 represents black.\n",
    "o_image = [\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe31198b-5d70-4f39-9ee5-af673732d1e7",
   "metadata": {},
   "source": [
    "Now, let's create an image of the letter **X** by creating a similar 6x6 matrix, where the 1s are now in an **X** pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b28183-d900-458e-aaef-28dfe9c7cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = [\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 0],\n",
    "    [0, 1, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 0, 0, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3661a-e60e-4cc0-a06b-efcafab2bc25",
   "metadata": {},
   "source": [
    "By squinting at the matrices we created for `o_image` and `x_image`, we can sort of see that we made them correctly. However, by drawing the images with matplotlib, we can get a much clearer picture, or rather, we can get much clearer pictures, of what we have done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d27f684-10dd-4887-962e-b89409f3e3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To draw the o_image and x_image, we first call subplots(), which creates \n",
    "# an array, called axarr[], with an entry for each element in a grid\n",
    "# specified by nrows and ncols.\n",
    "fig, axarr = plt.subplots(nrows=1, ncols=2, figsize=(4, 2))\n",
    "\n",
    "# Now we pass o_image and x_image to .imshow() for each element\n",
    "# in the grid created by plt.subplots()\n",
    "axarr[0].imshow(o_image, cmap='gray_r')  # Setting cmap='gray_r' or 'binary' gives us reverse grayscale.\n",
    "axarr[1].imshow(x_image, cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05fb7a5-812b-4838-97b1-00e2ae6f59d3",
   "metadata": {},
   "source": [
    "Now, let's put our training data into a **DataLoader**, which we can use to train the neural network. **DataLoaders** are great for large datasets because they make it easy to access the data in batches, make it easy to shuffle the data each epoch, and they make it easy to use a relatively small fraction of the data if we want to do a quick and dirty training for debugging our code.\n",
    "\n",
    "To put our data training data into a **DataLoader**, we start by converting the images into tensors with `torch.tensor()` and save them as `input_images`.\n",
    "\n",
    "_NOTE: When we call `torch.tensor()`, we tack on `type(torch.float32)` to ensure the numbers are saved in the correct format for the neural network to process efficiently._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0b7cbe-71f3-4d03-b291-bcd0e0efd138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the images into tensors...\n",
    "input_images = torch.tensor([o_image, x_image]).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8dc01-dfe3-4583-99c4-1f3c07c30dfe",
   "metadata": {},
   "source": [
    "Then, we create tensors for the labels, the ideal output values given each input image. In this example, our convolutional neural network has 2 outputs, one for the letter **O** and one for the letter **X**, so `[1.0, 0.0]` will represent the ideal output for the letter **O**, and `[0.0, 1.0]` will represent the ideal output for the letter **X**. The labels will be saved in `input_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4da9669-2834-4702-a708-fe357d0f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the labels for the input images\n",
    "input_labels = torch.tensor([\n",
    "    [1.0, 0.0],   # Codes for label 0\n",
    "    [0.0, 1.0]],  # Codes for label 1\n",
    "dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9fef8-7737-4262-acbf-f7307d0015bb",
   "metadata": {},
   "source": [
    "Finally, we combine `input_images` with `input_labels` to create a **TensorDataset** and use the **TensorDataset** to create the **DataLoader**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e4d801-a6e2-42c6-9c68-279a0555a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine input_images and input_labels into a TensorDataset\n",
    "dataset = TensorDataset(input_images, input_labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca312e-fb67-4973-8009-9cf0be837a20",
   "metadata": {},
   "source": [
    "Now, just for fun, we can verify that `dataloader` contains the input images and labels by using it in a `for` loop and printing out the images and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ce7d8f-c2a5-4859-92ed-30beb59f9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num: 0\n",
      "tensor([[[0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.]]])\n",
      "tensor([[1., 0.]])\n",
      "\n",
      "batch_num: 1\n",
      "tensor([[[1., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 0., 1., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0., 0., 1.]]])\n",
      "tensor([[0., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (images, labels) in enumerate(dataloader):\n",
    "    print(\"batch_num:\", batch_num)\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    print()  # print a blank line to separate each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31955e37-156e-4c5b-b4bd-806d0e6e2d74",
   "metadata": {},
   "source": [
    "At long last, we have created the **DataLoader** that we will use to train a convolutional neural network. Now, let's build it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea",
   "metadata": {},
   "source": [
    "## Building a Convolutional Neural Network (CNN)\n",
    "\n",
    "Building a CNN with PyTorch means creating a new class. And to make it easy to train the neural network, this class will inherit from `LightningModule`.\n",
    "\n",
    "Our new class will have the following methods:\n",
    "\n",
    "- `__init__()` to initialize the weights and biases and keep track of a few other housekeeping things.\n",
    "- `forward()` to make a forward pass through the convolutional neural network.\n",
    "- `configure_optimizers()` to configure the optimizer. In this tutorial, we'll use `Adam`.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss, and keep track of the loss values in a log file.\n",
    "\n",
    "Here is a picture of the neural network we want to create:\n",
    "\n",
    "<img src=\"./images/full_cnn.png\" alt=\"a simple convolutional neural network\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f26851-96df-4a84-b1ce-3351e61adf25",
   "metadata": {},
   "source": [
    "As we can see in the picture, our convolutional neural network accepts a 6x6 image as the input. Then, it applies a 3x3 **filter**, or **kernel**, to the image. That is the **convolutional** step in a Convolutional Neural Network. Anyway, then we run the output from the filter through a **ReLU** activation function before a **max pooling** step. Lastly, we run the max pooling values through a *simple neural network* with 4 inputs and 2 outputs.\n",
    "\n",
    "Don't forget that all of them are being determined or optimized after backpropagation: the kernel matrix (that is initiated with random values), the biais to its output ant that goes to the **feature map**, then the ReLU function setting all the negative values to zero. Finally a filter will select the maximum value in non-overlaping squares (max pooling). This will be converted to a column tensor and will be used as an input to a standard neural network, with weights and biaises\n",
    "\n",
    "_NOTE: this specific convolutional neural network is super simple. Usually, the images have multiple color channels (one for red, one for blue, and one for green), and multiple filters are applied to each channel. The output from these filters can then be run through multiple layers of filters. In contrast, our example is super simple, so we'll call it `SimpleCNN`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7879b36c-947f-4638-9287-d6d1d50e734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        ################################################################\n",
    "        # Here is where we initialize the Weights and Biases for the CNN\n",
    "        ################################################################\n",
    "\n",
    "        # The filter is created and applied by nn.Conv2d().\n",
    "        # in_channels - The number of color channels that\n",
    "        #    the image has. Our black and white image only \n",
    "        #    has one channel. However, color pictures usually have 3.\n",
    "        # out_channels - If we had multiple input channels, we could merge\n",
    "        #    them down to one output. Or we can increase the number of\n",
    "        #    output channels if we want.\n",
    "        # kernel_size - The size of the filter (aka kernel). In this case\n",
    "        #    we want a 3x3 filter, but you can select all kinds of sizes,\n",
    "        #    including sizes that are more rectangular than square.\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "\n",
    "        # nn.MaxPool2d() does the max pooling step.\n",
    "        # kernel_size - The size of the filter (aka kernel) that does the\n",
    "        #    max pooling. We're using a 2x2 grid for our filter.\n",
    "        # stride - How much to move the filter each step. In this case\n",
    "        #    we're moving it 2 units. Thus, our 2x2 filter does max pooling\n",
    "        #    before moving 2 units over (or down). This means that our \n",
    "        #    max pooling filter never overlaps itself.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Lastly, we create the \"normal\" neural network that has\n",
    "        # 4 inputs, in_features=4, going to a single activation function, out_features=1, \n",
    "        # in a single hidden layer.\n",
    "        self.input_to_hidden = nn.Linear(in_features=4, out_features=1)\n",
    "        # and the single hidden layer, in_features=1, goes to\n",
    "        # two outputs, out_features=2\n",
    "        self.hidden_to_output = nn.Linear(in_features=1, out_features=2)\n",
    "\n",
    "        # We'll use Cross Entropy to calculate the loss between what the \n",
    "        # neural network's predictions and actual, or known, species for\n",
    "        # each row in the dataset.\n",
    "        # NOTE: nn.CrossEntropyLoss applies a SoftMax function to the values\n",
    "        # we give it, so we don't have to do that oursevles. However,\n",
    "        # when we use this neural network (after it has been trained), we'll\n",
    "        # have to remember to apply a SoftMax function to the output.\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # First we apply a filter to the input image\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Then we run the output from the filter through a ReLU\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Then we run the output from the ReLU through a Max Pooling layer\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Now, at this point we have a square matrix of values.\n",
    "        # So, in order to use those values as inputs to\n",
    "        # a neural network, we use torch.flatten() to \n",
    "        # turn the matrix into a vector.\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "\n",
    "        # Now we run the flattened values through a neural network\n",
    "        # with a single hidden layer and a single ReLU activation\n",
    "        # function in that layer.\n",
    "        x = self.input_to_hidden(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_to_output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # In this example, configuring the optimizer\n",
    "        # consists of passing it the weights and biases we want\n",
    "        # to optimize, which are all in self.parameters(),\n",
    "        # and setting the learning rate with lr=0.001.\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # The first thing we do is split 'batch'\n",
    "        # into the input and label values.\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Then we run the input through the neural network\n",
    "        outputs = self.forward(inputs)\n",
    "\n",
    "        # Then we calculate the loss.\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        # Lastly, we could add the loss to a log file\n",
    "        # so that we can graph it later. This would\n",
    "        # help us decide if we have done enough training\n",
    "        # Ideally, if we do enough training, the loss\n",
    "        # should be small and not getting any smaller.\n",
    "        # self.log(\"loss\", loss)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46a88b",
   "metadata": {},
   "source": [
    "Note that we can refactor the class using `nn.Sequential` and modules, e.g., ReLu and flatten, instead of functions, and make the code a bit cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772d744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        # We define the entire network as one sequential block\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # 1. Convolutional Part\n",
    "            # (Corresponds to self.conv(x) and F.relu(x))\n",
    "            nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # 2. Pooling Part\n",
    "            # (Corresponds to self.pool(x))\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # 3. Flattening Part\n",
    "            # (Corresponds to torch.flatten(x, 1))\n",
    "            # This converts the 2D feature map into a 1D vector\n",
    "            nn.Flatten(start_dim=1), # start_dim=1 keeps the batch dimension\n",
    "\n",
    "            # 4. Fully-Connected (Classifier) Part\n",
    "            # (Corresponds to the rest of the original forward pass)\n",
    "            nn.Linear(in_features=4, out_features=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1, out_features=2)\n",
    "        )\n",
    "        \n",
    "        # The loss function remains the same\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward pass is now just one clean line\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # This method remains exactly the same\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        # This method remains exactly the same\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a3cac-ccb9-49d2-96fc-b797d7e0823c",
   "metadata": {},
   "source": [
    "## Training the CNN\n",
    "\n",
    "Training our new convolutional neural network means we create a model from the new class, `SimpleCNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cb78b2a-cd0b-4072-97cb-664259e5aeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353a83-3690-4a36-a164-e60e499bc79f",
   "metadata": {},
   "source": [
    "And then create a **Lightning Trainer**, `L.Trainer()`, and use it to optimize the parameters. _NOTE: We will start with 100 epochs, 100 complete runs through our training data. This may be enough to successfully optimize all of the parameters, but it might not._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b994c8-46d0-4f06-a561-c49a75f5fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | net  | Sequential       | 19     | train\n",
      "1 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b818f486884907afd86a78e5256cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f",
   "metadata": {},
   "source": [
    "Now, let's see if the predictions are any good. We can do this by seeing how well it classifies the original training data. And we can do this using a `for` loop, just like we did earlier when we used to print out the contents of the **DataLoader** that we used for training.\n",
    "\n",
    "The **CrossEntropy** loss function we use during training applies a SoftMax function to the outputs. Thus, now that we're using the model, we'll apply `torch.softmax()` to its output. This will make it easier to read and interpret. And to make it even easier to read, we can run it through `torch.round()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f92d56c6-498d-47f6-adc4-ada1d52dfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.5200, 0.4800]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "predicted_label: tensor([[0.4200, 0.5800]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    # First, run the image through the model to make a prediction\n",
    "    prediction = model(image)\n",
    "\n",
    "    # Now make the prediction easy to read and interpret by\n",
    "    # running it through torch.softmax() and torch.round()\n",
    "    predicted_label = torch.round(\n",
    "        torch.softmax(prediction, dim=1),  # dim=0 applies softmax to rows, dim=1 to columns\n",
    "        decimals=2)\n",
    "\n",
    "    # Now print out the the predicted label and the original label\n",
    "    # so we see how well our CNN performed.\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261fdd41-2caa-43be-8c78-0c2d64f08f3d",
   "metadata": {},
   "source": [
    "As we can see in the output printed above, after 100 epochs of training, our convolutional neural network is not making good predictions. So let's do a lot more training.\n",
    "\n",
    "Here again, we can pick up where we left off training without starting over from scratch. This is because training with **Lightning** creates _checkpoint_ files that keep track of the Weights and Biases as they change. As a result, all we have to do to pick up where we left off is tell the `Trainer` where the checkpoint files are. This will save us a lot of time since we don't have to retrain the first **100** epochs. So, let's add an additional **600** epochs to the training.\n",
    "\n",
    "To add additional epochs to the training, we first identify where the checkpoint file is with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bc4c1b-911d-417f-8ab2-9f52712502e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path  # By default, \"best\" = \"most recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fd497-997c-4369-9b3e-d25cd0911d5d",
   "metadata": {},
   "source": [
    "Then we create a new **Lightning Trainer**, just like before, but we set the number of epochs to 700. Given that we already trained for 100 epochs, this means we'll do 600 more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a849d2f-bad1-4214-a72b-f1b04174639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_06\\lightning_logs\\version_4\\checkpoints\\epoch=99-step=200.ckpt\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:445: The dirpath has changed from 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_06\\\\lightning_logs\\\\version_4\\\\checkpoints' to 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_06\\\\lightning_logs\\\\version_5\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | net  | Sequential       | 19     | train\n",
      "1 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_06\\lightning_logs\\version_4\\checkpoints\\epoch=99-step=200.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7227642dccd14447819794b7909dcde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=700` reached.\n"
     ]
    }
   ],
   "source": [
    "# First, create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=700)\n",
    "\n",
    "# Then call trainer.fit() using the path to the most recent checkpoint files\n",
    "# so that we can pick up where we left off.\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e",
   "metadata": {},
   "source": [
    "Now, let's see if the predictions have improved after 700 epochs. We'll do this using the same `for` loop that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cabdaf11-e107-4b3e-bda8-d6b8d3d820d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label: tensor([[0.7900, 0.2100]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[1., 0.]])\n",
      "predicted_label: tensor([[0.0100, 0.9900]], grad_fn=<RoundBackward1>)\n",
      "original label: tensor([[0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "for batch_num, (image, label) in enumerate(dataloader):\n",
    "    \n",
    "    prediction = model(image)\n",
    "\n",
    "    predicted_label = torch.round(\n",
    "        torch.softmax(prediction, dim=1),\n",
    "        decimals=2)\n",
    "\n",
    "    print(\"predicted_label:\", predicted_label)\n",
    "    print(\"original label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31918cf1-5cfc-475b-b61a-f08525111741",
   "metadata": {},
   "source": [
    "After 700 training epochs, the predicted values are much, much closer to the ideal values. And if we rounded them to the nearest whole number, we'd get the exact labels that we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098",
   "metadata": {},
   "source": [
    "## Make a Prediction with New Data\n",
    "\n",
    "Now that our model is trained, we can use it to make predictions from new data. Specifically, let's see how well our model predicts an image of the letter **X** that has been shifted over one pixel. First, let's create the image of an **X** shifted by one pixel to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f2a2fe8-3418-4adc-93ea-a95ecb717b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_x_image = [\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "    [0, 0, 1, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe61a6-a644-419c-a23f-17b1415dd973",
   "metadata": {},
   "source": [
    "Now, let's verify that we created the correct image by drawing with matplotlib, just like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ad7c2a-ff39-4cfb-bb69-2722b723db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.imshow(shifted_x_image, cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed44d62a-d54c-41c6-b73a-1301b4a6d003",
   "metadata": {},
   "source": [
    "Now that we can see that we correctly created an image of an **X** shifted one pixel to the right, let's see whether or not our trained convolutional neural network can correctly classify it as an **X**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "616a977c-ffc3-43b0-8f33-096a8284c162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1200, 0.8800]], grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, let's make a prediction with the new image\n",
    "prediction = model(\n",
    "    torch.tensor([shifted_x_image]).type(torch.float32))\n",
    "\n",
    "# Now make the prediction easy to read and interpret by\n",
    "# running it through torch.softmax() and torch.round()\n",
    "predicted_label = torch.round(\n",
    "    torch.softmax(prediction, dim=1),\n",
    "    decimals=2)\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf5033-422d-4731-bedb-d49b4e82e47b",
   "metadata": {},
   "source": [
    "And we see that the trained network correctly predicted **X** since the second output value, which represents **X**, is larger than the first output value, which represents **O**.\n",
    "\n",
    "Now, let's see if our trained network can correctly classify an image of the letter **O** that is shifted one pixel to the left. We'll do this just like we did for the shifted **X**. So, the first thing we do is create the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32266790-ceda-4ab0-a5e9-2888961c2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_o_image = [\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 0],\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 1, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f3ba40a-7912-44e8-96f8-9bfea66c56dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(2, 2))\n",
    "ax.imshow(shifted_o_image, cmap='gray_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e1c572-113a-4685-bef7-2926783b21c9",
   "metadata": {},
   "source": [
    "Lastly, we run the image through the trained network to make a prediction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e849cbb-d3e2-4e2a-a7cd-960db3d0630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4500, 0.5500]], grad_fn=<RoundBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(\n",
    "    torch.tensor([shifted_o_image]).type(torch.float32))\n",
    "\n",
    "predicted_label = torch.round(\n",
    "    torch.softmax(prediction, dim=1),\n",
    "    decimals=2)\n",
    "\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f076ae9-7efb-4ffb-ae6f-3793d28fb655",
   "metadata": {},
   "source": [
    "And it looks like our network thought that the shifted **O** was an **X**. Oh well...\n",
    "\n",
    "However, we learned a lot, and with more training data and more variation in the letters, we might be able to make better predictions in the future!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
