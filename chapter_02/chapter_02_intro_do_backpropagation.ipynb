{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d",
      "metadata": {
        "id": "61a3466e-96aa-47bb-a7af-b8e9a93d3d5d"
      },
      "source": [
        "# Chapter 2 -- Build and train a very simple neural network with backpropagation\n",
        "\n",
        "In this notebook, we will build, and train the very simple neural network featured in Chapter 2 of **[The StatQuest Illustrated Guide to Neural Networks and AI](https://www.amazon.com/dp/B0DRS71QVQ)**. We'll start with the untrained **Weights** and **Biases**, as seen in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_train_all.png?raw=1\" alt=\"an untrained neural network\" style=\"width: 800px;\">\n",
        "\n",
        "Then train them with **Backpropagation** to get the trained Weights and Biases seen in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_1_pre_trained_nn_labeled.png?raw=1\" alt=\"a trained neural network\" style=\"width: 800px;\">\n",
        "\n",
        "In this tutorial, we will:\n",
        "\n",
        "- **Code a neural network with untrained weights and biases**. This will show the basic structure of a class that inherits from [`LightningModule` to build a neural network](https://lightning.ai/docs/pytorch/stable/).\n",
        "- **Train the weights and biases in our neural network**. This will show how to train the weights and biases in a simple neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c",
      "metadata": {
        "id": "8c818152-6ca2-4ee0-9168-f77c9b4cac1c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "# see https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        "# SGD is short of Stochastic Gradient Descent, but\n",
        "# the way we'll use it, passing in all of the training\n",
        "# data at once instead of passing it random subsets,\n",
        "# it will act just like plain old Gradient Descent.\n",
        "\n",
        "import lightning as L  ## Lightning makes it easier to write, optimize and scale code\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "## We'll store our data in DataLoaders. See explanations below.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"]= (2,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d471f59",
      "metadata": {},
      "source": [
        "*PyTorch* is the foundational deep learning framework. It provides the fundamental tools to build and define neural networks, like tensors and layers.\n",
        "\n",
        "*PyTorch Lightning* is a high-level wrapper built on top of PyTorch. It's not a replacement; it's a way to **organize** the PyTorch code and remove boilerplate. It enforces a clean structure using the `LightningModule`. This class organizes the code into clear, dedicated methods:\n",
        "\n",
        "_ `__init__()` & `forward()`: define the model layers (just like in pure PyTorch).\n",
        "- `training_step()`: what happens for one batch of training.\n",
        "- `validation_step()`: what happens for one batch of validation.\n",
        "- `test_step()`: what happens for one batch of testing.\n",
        "- `configure_optimizers()`: where we return our optimizer (e.g., Adam)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa",
      "metadata": {
        "id": "b4ca5fb5-e879-4453-9e86-dbc37c158afa"
      },
      "source": [
        "## Create the Training Dataset\n",
        "\n",
        "In Chapter 2, we had a very simple dataset that consisted of three points, as seen in the figure below.\n",
        "\n",
        "<img src=\"https://github.com/StatQuest/signa/blob/main/chapter_02/images/chapter_2_training_data.png?raw=1\" alt=\"a simple dataset for training\" style=\"width: 800px;\">\n",
        "\n",
        "Although it's not required, we're going to put our training data into a `DataLoader`. This will offer a lot of features, for example, if we had a large dataset, a `DataLoader` gives us an easy way to access the data in batches instead of all at once. This is critical when we have more data than RAM to store it in. A DataLoader can also shuffle the data for us each epoch and makes it easy to only use a fraction of the data if we want to do a quick and rough training for debugging purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49840acf-ddb6-4577-8401-de872ab62c32",
      "metadata": {
        "id": "49840acf-ddb6-4577-8401-de872ab62c32"
      },
      "outputs": [],
      "source": [
        "# The inputs are the x-axis coordinates for each data point\n",
        "# These values represent different doses\n",
        "training_inputs = torch.tensor([0.0, 0.5, 1.0])\n",
        "\n",
        "# The labels are the y-axis coordinates for each data point\n",
        "# These values represent the effectiveness\n",
        "training_labels = torch.tensor([0.0, 1.0, 0.0])\n",
        "\n",
        "# Now let's package everything up into a DataLoader\n",
        "training_dataset = TensorDataset(training_inputs, training_labels)\n",
        "dataloader = DataLoader(training_dataset)  # Feed with a tuple of tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "107f6ac0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0.0000, 0.5000, 1.0000]), tensor([0., 1., 0.]))\n"
          ]
        }
      ],
      "source": [
        "print(training_dataset.tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adfb2eb-0ad7-426d-9946-6a7185737647",
      "metadata": {
        "id": "7adfb2eb-0ad7-426d-9946-6a7185737647"
      },
      "source": [
        "## Create a Neural Network\n",
        "\n",
        "Now we'll build a neural network that has trainable Weights and Biases.\n",
        "\n",
        "For this, we'll use `L.LightningModule`, which has everything `nn.Module` has, plus we can define the optimizer we want to use as well as tell PyTorch how each training step should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4881afee-1cd9-4440-8ffc-1f86b321c539",
      "metadata": {
        "id": "4881afee-1cd9-4440-8ffc-1f86b321c539"
      },
      "outputs": [],
      "source": [
        "class myNN(L.LightningModule):  # We don't henerit from `nn.Module` like in Chapter 1.\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Create all of the weights and biases for the network.\n",
        "        However, this time they are initialized with *random values*.\n",
        "\n",
        "        We are also wrapping the tensors up in `nn.Parameter()`\n",
        "        objects, instead of just tensor, like previously , for example\n",
        "        in `self.w1 = torch.tensor(1.43)`. PyTorch will only optimize \n",
        "        parameters. There are a lot of different ways to create parameters,\n",
        "        and we'll see those in later examples, but `nn.Parameter()` is \n",
        "        the most basic.\n",
        "        \"\"\"\n",
        "\n",
        "        self.w1 = nn.Parameter(torch.tensor(0.06))\n",
        "        self.b1 = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "        self.w2 = nn.Parameter(torch.tensor(3.49))\n",
        "        self.b2 = nn.Parameter(torch.tensor(0.0))\n",
        "\n",
        "        self.w3 = nn.Parameter(torch.tensor(-4.11))\n",
        "        self.w4 = nn.Parameter(torch.tensor(2.74))\n",
        "\n",
        "        self.loss = nn.MSELoss(reduction='sum')  # SSR\n",
        "\n",
        "\n",
        "    def forward(self, input_values):  # Identical to what we used in Chapter 1.\n",
        "\n",
        "        top_x_axis_values = (input_values * self.w1) + self.b1\n",
        "        bottom_x_axis_values = (input_values * self.w2) + self.b2\n",
        "\n",
        "        top_y_axis_values = F.relu(top_x_axis_values)\n",
        "        bottom_y_axis_values = F.relu(bottom_x_axis_values)\n",
        "\n",
        "        output_values = (top_y_axis_values * self.w3) + (bottom_y_axis_values * self.w4)\n",
        "\n",
        "        return output_values\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):  # Configures optimizer to use for backpropagation.\n",
        "\n",
        "        \"\"\"\n",
        "        PyTorch doesn't have a Gradient Descent optimizer (see page 51), but a\n",
        "        Stochastic Gradient Descent (SGD) optimizer, that uses randoms subsets of \n",
        "        data. However, since we are running only 3 doses through the NN each time, \n",
        "        rather than a random subset, we are essentially doing Gradient Descent.\n",
        "        \"\"\"\n",
        "\n",
        "        return SGD(self.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "    def training_step(self, batch):  # Take a step during gradient descent.\n",
        "        \n",
        "        inputs, labels = batch  # Collect input\n",
        "        outputs = self.forward(inputs)  # Run input through the neural network\n",
        "        loss = self.loss(outputs, labels)\n",
        "\n",
        "        \"\"\"\n",
        "        the `loss` quantifies the difference between the observed drug effectiveness\n",
        "        in `labels` and the outputs created by the neural network.\n",
        "        \"\"\"\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ac63e3",
      "metadata": {},
      "source": [
        "PyTorch Lightning operates on a principle of \"convention over configuration.\" It expects us to define specific methods with particular names in the `LightningModule`, and it will automatically call them at the correct time during the training, validation, and testing loops.\n",
        "\n",
        "The `training_step` method is one of these required, conventional methods for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a",
      "metadata": {
        "id": "ff36d865-fcab-435d-9b1d-484a3ed8cd7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w1 tensor(0.0600)\n",
            "b1 tensor(0.)\n",
            "w2 tensor(3.4900)\n",
            "b2 tensor(0.)\n",
            "w3 tensor(-4.1100)\n",
            "w4 tensor(2.7400)\n"
          ]
        }
      ],
      "source": [
        "model = myNN()  # Make model from the class\n",
        "\n",
        "# Now print out the name and value for each named parameter parameter in the model. \n",
        "# Remember parameters are variables, like Weights and Biases, that we can train.\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, torch.round(param.data, decimals=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "033dd021",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor(0.0600, requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor(0., requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor(3.4900, requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor(0., requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor(-4.1100, requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor(2.7400, requires_grad=True)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a812dc07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: myNN(\n",
            "  (loss): MSELoss()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Model:\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa",
      "metadata": {
        "id": "9bd1842d-b97c-4b8f-91c9-6bc6884e1dfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output from the initialized NN:\n",
            "tensor([0., 5., 9.], grad_fn=<RoundBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Run different doses through the neural network through `forward`\n",
        "output_values = model(training_inputs)\n",
        "\n",
        "print(\"Output from the initialized NN:\")\n",
        "print(torch.round(output_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29",
      "metadata": {
        "id": "e0c0a5a5-47f0-4e45-be2b-a7cf10756e29"
      },
      "source": [
        "## Plot of the initial model\n",
        "\n",
        "We successfully ran the doses from the training data through the model. However, the output from the model is way different than we expect (we expected 0.0, 1.0, and 0.0). So let's draw a picture of the bent shape that the model uses to make predictions and compare that to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "901253af-43f6-40dc-97af-175c36e5290e",
      "metadata": {
        "id": "901253af-43f6-40dc-97af-175c36e5290e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input dose values: tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
            "        0.9000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "# Create the different doses we want to run through the neural network.\n",
        "# torch.linspace() creates the sequence of numbers between, and including, 0 and 1.\n",
        "input_doses = torch.linspace(start=0, end=1, steps=11)\n",
        "\n",
        "# Print out the doses to make sure they are what we expect\n",
        "print(\"Input dose values:\", input_doses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a",
      "metadata": {
        "id": "c84cbadf-86ec-4f11-af2c-dfece12f197a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output predicted effectiveness: tensor([0.0000, 0.9316, 1.8632, 2.7948, 3.7264, 4.6580, 5.5896, 6.5212, 7.4528,\n",
            "        8.3844, 9.3160], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output_values = model(input_doses)\n",
        "print(\"Output predicted effectiveness:\", output_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27eb86f",
      "metadata": {},
      "source": [
        "Now draw a graph that shows how well, or poorly, the model predicts the training data. At this point, since the model is untrained, there should be a big difference between the model's output and the training data.\n",
        "\n",
        "Note that every tensor that has `requires_grad=True` (like the current model) is part of a graph. PyTorch records every operation (addition, multiplication, etc.) involving this tensor. Libraries like Seaborn, Matplotlib, and NumPy have no concept of this computational graph. They are external libraries that expect simple data, like a Python number or a NumPy array, not a \"live\" PyTorch tensor that's actively tracking operations. That's why here we must first \"detach\" it from the computational graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a",
      "metadata": {
        "id": "f3bc01f1-65d4-4fb0-b428-6730f64d829a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# First, draw the individual output points\n",
        "sns.scatterplot(\n",
        "    x=input_doses,\n",
        "    # y=output_values.detach().numpy(),\n",
        "    y=output_values.detach(),\n",
        "    color='green',\n",
        "    label='model',\n",
        ")\n",
        "\n",
        "# Now connect those points with a line\n",
        "sns.lineplot(\n",
        "    x=input_doses,\n",
        "    y=output_values.detach(),\n",
        "    color='green',\n",
        "    linewidth=2)\n",
        "\n",
        "# Add the values in the training dataset\n",
        "sns.scatterplot(\n",
        "    x=training_inputs,\n",
        "    y=training_labels,\n",
        "    color='orange',\n",
        "    marker='s',\n",
        "    label='training data')\n",
        "\n",
        "# now label the y- and x-axes.\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746",
      "metadata": {
        "id": "89b10eaf-4fae-46bc-af6c-f05ebdada746"
      },
      "source": [
        "We see how badly the bent shape fits the training data, so let's train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd",
      "metadata": {
        "id": "59123d80-2a03-4f66-9a41-f3e85b040ddd"
      },
      "source": [
        "## Training the Weights and Biases\n",
        "\n",
        "Training consists of creating a **Lightning Trainer** with `L.Trainer()` and then calling the `fit()` method on the our model with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677",
      "metadata": {
        "id": "033b9bc6-39ac-4fb1-aabd-355f22f54677"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name         | Type    | Params | Mode \n",
            "-------------------------------------------------\n",
            "0 | loss         | MSELoss | 0      | train\n",
            "  | other params | n/a     | 6      | n/a  \n",
            "-------------------------------------------------\n",
            "6         Trainable params\n",
            "0         Non-trainable params\n",
            "6         Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "1         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
            "`Trainer.fit` stopped: `max_epochs=500` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer = L.Trainer(\n",
        "    max_epochs=500,  # How many times to go through the training data\n",
        "    logger=False,\n",
        "    enable_checkpointing=False,\n",
        "    enable_progress_bar=False)\n",
        "\n",
        "trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e099828",
      "metadata": {},
      "source": [
        "When we use a PyTorch Lightning `Trainer` to train our model, the `Trainer` executes a pre-defined, optimized training loop. Here's a simplified breakdown of what the `Trainer` does automatically behind the scenes for each batch of data:\n",
        "\n",
        "1.  **Fetches a Batch:** the `Trainer` gets a batch of data from the data loader. In the current example, it takes all 3 data points.\n",
        "2.  **Calls `training_step`:** it passes this batch to our model's `training_step(self, batch)` method. Remind here that the method returns `loss`.\n",
        "3.  **Receives the Loss:** the `Trainer` require that the `training_step` method returns a single tensor representing the calculated loss.\n",
        "4.  **Automatic Backpropagation:** once the `Trainer` receives the `loss` tensor from the defined `training_step`, it automatically performs the backpropagation process:\n",
        "    - Calling `loss.backward()` to compute the gradients of the loss with respect to all of our model's parameters (the `nn.Parameter` objects like `self.w1`, `self.b1`, etc.).\n",
        "    - Calling `optimizer.step()` to update the parameters based on the computed gradients and the learning rate. The optimizer itself is what we defined in the `configure_optimizers` method.\n",
        "    - Calling `optimizer.zero_grad()` to reset the gradients for the next iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "146b0cf3-9541-4d59-853f-63202fbad7ae",
      "metadata": {
        "id": "146b0cf3-9541-4d59-853f-63202fbad7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w1 tensor(1.4790)\n",
            "b1 tensor(-0.5680)\n",
            "w2 tensor(2.6980)\n",
            "b2 tensor(-0.2580)\n",
            "w3 tensor(-4.2400)\n",
            "w4 tensor(1.5830)\n"
          ]
        }
      ],
      "source": [
        "# Now that we've trained the model, let's print out the new values for each Weight and Bias.\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, torch.round(param.data, decimals=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3",
      "metadata": {
        "id": "bd1935b5-4256-49c9-9117-8aab8ad382b3"
      },
      "source": [
        "Lastly, let's draw a graph of the bent shape that the model is using for predictions and compare it to the training data. In theory, the bent shape should fit the data much better now that we have optimized the Weights and Biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2",
      "metadata": {
        "id": "11539eec-29cc-4efb-ab2e-ddf82ba360c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output predicted effectiveness (after training): tensor([0.0000, 0.0200, 0.4500, 0.8700, 1.2000, 1.0000, 0.8000, 0.6000, 0.4000,\n",
            "        0.2000, 0.0000], grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Run the different doses through the trained neural network.\n",
        "output_values_trained = model(input_doses)\n",
        "\n",
        "print(\n",
        "    \"Output predicted effectiveness (after training):\",\n",
        "    torch.round(output_values_trained, decimals=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cceac5",
      "metadata": {},
      "source": [
        "Now draw a graph that shows how well, or poorly, the model predicts the training data. At this point, since we just trained the model, the training data should overlap the model's output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "186b2d67-8621-4be3-937e-e1fc980475a9",
      "metadata": {
        "id": "186b2d67-8621-4be3-937e-e1fc980475a9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# First, draw the individual output points\n",
        "sns.scatterplot(\n",
        "    x=input_doses,\n",
        "    y=output_values_trained.detach(),\n",
        "    color='green',\n",
        "    label='predicted',\n",
        "    )\n",
        "\n",
        "# Now connect those points with a line\n",
        "sns.lineplot(\n",
        "    x=input_doses,\n",
        "    y=output_values_trained.detach(),\n",
        "    color='green',\n",
        "    linewidth=2)\n",
        "\n",
        "# Add the values in the training dataset\n",
        "sns.scatterplot(\n",
        "    x=training_inputs,\n",
        "    y=training_labels,\n",
        "    color='orange',\n",
        "    marker='s',\n",
        "    label='training'\n",
        "    )\n",
        "\n",
        "# Now label the y- and x-axes\n",
        "plt.ylabel('Effectiveness')\n",
        "plt.xlabel('Dose')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3598b8d4",
      "metadata": {},
      "source": [
        "## More mathematics\n",
        "\n",
        "### The neural network\n",
        "\n",
        "#### The unit neuron\n",
        "\n",
        "Basically, we can see the neural network as a mathematical function $y = f(X)$. With a single edge between two edges $a^{(0)}$, from the first layer $(0)$, and $a^{(1)}$ from the second layer $(1)$, both layers actually contain a single edge, we can write:\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Define the 'invisible' class for the bias\n",
        "    classDef biasStyle fill:none,stroke:none,color:#000\n",
        "    \n",
        "    subgraph Input Layer\n",
        "        direction TB\n",
        "        I((\"aâ½â°â¾\"))\n",
        "    end\n",
        "\n",
        "    subgraph Output Layer\n",
        "        direction TB\n",
        "        O((\"aâ½Â¹â¾\"))\n",
        "    end\n",
        "\n",
        "    S{\"Î£\"}\n",
        "    f[\"Ï\"]\n",
        "    b[\"b\"]:::biasStyle\n",
        "\n",
        "    I -->|w| S\n",
        "    b --> S\n",
        "    S --> f\n",
        "    f --> O\n",
        "```\n",
        "\n",
        "$$\n",
        "a^{(0)} \\longmapsto a^{(1)} \\\\\n",
        "a^{(1)} = \\sigma(wa^{(0)} + b)\n",
        "$$\n",
        "\n",
        "with:\n",
        "\n",
        "- $a$ the activity\n",
        "- $w$ the weigth\n",
        "- $b$ the biais\n",
        "- $\\sigma$ the activation function, e.g., ReLU, that receives information and is activated above a certain threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445b04bc",
      "metadata": {},
      "source": [
        "#### Towards vectors\n",
        "\n",
        "Let's say now that we have two nodes in the input layer $(0)$, i.e., $a_0^{(0}$ and $a_1^{(0}$, with two edges leading to a single node in the output layer $(1)$, i.e., $a^{(1)}$.\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Define the 'invisible' class for the bias\n",
        "    classDef biasStyle fill:none,stroke:none,color:#000\n",
        "\n",
        "    subgraph Input Layer\n",
        "        direction TB\n",
        "        I0((\"aââ½â°â¾\"))\n",
        "        I1((\"aââ½â°â¾\"))\n",
        "    end\n",
        "\n",
        "    subgraph Output Layer\n",
        "        direction TB\n",
        "        O((\"aâ½Â¹â¾\"))\n",
        "    end\n",
        "\n",
        "    S{\"Î£\"}\n",
        "    f[\"Ï\"]\n",
        "    b[\"b\"]:::biasStyle\n",
        "\n",
        "    I0 -->|\"wâ\"| S\n",
        "    I1 -->|\"wâ\"| S\n",
        "    b --> S\n",
        "\n",
        "    S --> f\n",
        "    f --> O\n",
        "```\n",
        "\n",
        "Then we can write:\n",
        "\n",
        "$$\n",
        "a^{(1)} = \\sigma(w_0a_0^{(0)} + w_1a_1^{(0)} + b) \\\\\n",
        "a^{(1)} = \\sigma\\left(\\sum_{j=0}^{1}(w_ja_j^{(0)}) + b\\right) \\\\\n",
        "$$\n",
        "\n",
        "Using vector notation for weights and input, we can even rewrite it as a dot product:\n",
        "\n",
        "$$\n",
        "a^{(1)} = \\sigma\\left(\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix} \\cdot \\begin{bmatrix} a_0^{(0)} \\\\ a_1^{(0)} \\end{bmatrix} + b \\right) \\\\\n",
        "a^{(1)} = \\sigma(\\mathbf{w} \\cdot \\mathbf{a}^{(0)} + b)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a2c9911",
      "metadata": {},
      "source": [
        "#### Towards matrices\n",
        "\n",
        "A neuron actually consists of the sum of different weighted input, in addition to a biais, followed by an activation function, altogether leading to the output.\n",
        "\n",
        "![Graphical representation of a neuron](https://blogs.cornell.edu/info2040/files/2015/09/VqOpE-1c4xc4y.jpg)\n",
        "\n",
        "Expanding the concept to a more complex neural networks, for example, where each neuron has its own bias, and when there are multiple nodes in the output layer as in the example below. _Note that we don't shown explicitely the sum and activation function for clarity._\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Define the 'invisible' class for the bias\n",
        "    classDef biasStyle fill:none,stroke:none,color:#000\n",
        "\n",
        "    I0((\"aââ½â°â¾\"))\n",
        "    I1((\"aââ½â°â¾\"))\n",
        "    I2((\"aââ½â°â¾\"))\n",
        "\n",
        "    O0((\"aââ½Â¹â¾\"))\n",
        "    O1((\"aââ½Â¹â¾\"))\n",
        "\n",
        "    b0[\"bââ½Â¹â¾\"]:::biasStyle\n",
        "    b1[\"bââ½Â¹â¾\"]:::biasStyle\n",
        "\n",
        "    I0 -->|\"wâ,ââ½Â¹â¾\"| O0\n",
        "    I0 -->|\"wâ,ââ½Â¹â¾\"| O1\n",
        "    I1 -->|\"wâ,ââ½Â¹â¾\"| O0\n",
        "    I1 -->|\"wâ,ââ½Â¹â¾\"| O1\n",
        "    I2 -->|\"wâ,ââ½Â¹â¾\"| O0\n",
        "    I2 -->|\"wâ,ââ½Â¹â¾\"| O1\n",
        "\n",
        "    b0 -.-> O0\n",
        "    b1 -.-> O1\n",
        "```\n",
        "\n",
        "In this example, the two output vectors can be written as:\n",
        "\n",
        "$$\n",
        "a_0^{(1)} = \\sigma(\\mathbf{w}_0^{(1)} \\cdot \\mathbf{a}^{(0)} + b_0^{(1)})\n",
        "\\quad\n",
        "a_1^{(1)} = \\sigma(\\mathbf{w}_1^{(1)} \\cdot \\mathbf{a}^{(0)} + b_1^{(1)})\n",
        "$$\n",
        "\n",
        "At the end, we can switch to matrix (weights) and vector (output, input and bias) notations and write:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} a_0^{(1)} \\\\ a_1^{(1)} \\end{bmatrix} = \\sigma\\left(\n",
        "    \\begin{bmatrix}\n",
        "    w_{0,0}^{(1)} && w_{1,0}^{(1)} && w_{2,0}^{(1)} \\\\\n",
        "    w_{0,1}^{(1)} && w_{1,1}^{(1)} && w_{2,1}^{(1)}\n",
        "    \\end{bmatrix}\n",
        "\n",
        "    \\begin{bmatrix}\n",
        "    a_0^{(0)} \\\\\n",
        "    a_1^{(0)} \\\\\n",
        "    a_2^{(0)}\n",
        "    \\end{bmatrix}\n",
        "\n",
        "    + \\begin{bmatrix}\n",
        "    b_0^{(1)} \\\\\n",
        "    b_1^{(1)}\n",
        "    \\end{bmatrix}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "Generalizing further, for example for a network consisting of two layers, with $n$ input and $m$ output nodes, we can write:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(1)} = \\sigma\\left(\\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}\\right)\n",
        "$$\n",
        "\n",
        "with the vector $\\mathbf{a}^{(1)}$ of size $[m \\times 1]$, same for vector $\\mathbf{b}^{(1)}$, the vector $\\mathbf{a}^{(0)}$ of size $[n \\times 1]$, and finally the matrix $\\mathbf{W}^{(1)}$ of size $[m \\times n]$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9130351b",
      "metadata": {},
      "source": [
        "#### More complex network\n",
        "\n",
        "Let's now look at a more complex neural network with a hidden layer.\n",
        "\n",
        "```mermaid\n",
        "%%{init:{'theme': 'neutral'}}%%\n",
        "graph LR\n",
        "    %% Input Layer (4 nodes)\n",
        "    subgraph \"Input aâ½â°â¾\"\n",
        "        direction TB\n",
        "        I1(( ))\n",
        "        I2(( ))\n",
        "        I3(( ))\n",
        "        I4(( ))\n",
        "    end\n",
        "\n",
        "    %% Hidden Layer (3 nodes)\n",
        "    subgraph \"Hidden aâ½Â¹â¾\"\n",
        "        direction TB\n",
        "        H1(( ))\n",
        "        H2(( ))\n",
        "        H3(( ))\n",
        "    end\n",
        "\n",
        "    %% Output Layer (2 nodes)\n",
        "    subgraph \"Output aâ½Â²â¾\"\n",
        "        direction TB\n",
        "        O1(( ))\n",
        "        O2(( ))\n",
        "    end\n",
        "\n",
        "    %% Connections from Input to Hidden\n",
        "    I1 --> H1\n",
        "    I1 --> H2\n",
        "    I1 --> H3\n",
        "    I2 --> H1\n",
        "    I2 --> H2\n",
        "    I2 --> H3\n",
        "    I3 --> H1\n",
        "    I3 --> H2\n",
        "    I3 --> H3\n",
        "    I4 --> H1\n",
        "    I4 --> H2\n",
        "    I4 --> H3\n",
        "\n",
        "    %% Connections from Hidden to Output\n",
        "    H1 --> O1\n",
        "    H1 --> O2\n",
        "    H2 --> O1\n",
        "    H2 --> O2\n",
        "    H3 --> O1\n",
        "    H3 --> O2\n",
        "```\n",
        "\n",
        "Hidden layers behave the same, and the outputs of the previous layer become the inputs of the next layer.\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(1)} = \\sigma\\left(\\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}\\right) \\\\\n",
        "\\mathbf{a}^{(2)} = \\sigma\\left(\\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}\\right)\n",
        "$$\n",
        "\n",
        "So that for a given layer $L$, we can write:\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{(L)} = \\sigma\\left(\\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4abc65b0",
      "metadata": {},
      "source": [
        "### The cost function\n",
        "\n",
        "Generally, the cost function can be written as:\n",
        "\n",
        "$$\n",
        "C = \\sum_i (a_i^{(L)} - y_i)^2\n",
        "$$\n",
        "\n",
        "where $a_i^{(L)}$ is the output vector given by the network, and $y_i$ the desired output. Basically we want to minimize the difference between the true/expected output and the output of the current network, by finding the minimum of the multidimensional hypersurface.\n",
        "\n",
        "This where the derivatives come into the game, and in particular the [Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), which for a function $f(x_1, x_2, x_3, \\dots)$ is the row vector $J$ such as $J = [\\frac{\\partial{f}}{\\partial{x_1}}, \\frac{\\partial{f}}{\\partial{x_2}}, \\frac{\\partial{f}}{\\partial{x_3}}, \\dots]$. It gatters together the partial derivatives of the cost function with respect to all relevant variables, i.e., the weight and bias values. We can use the Jacobian for drawing the Jacobian vector field at different point of the hyperspace.\n",
        "\n",
        "Let's write the following equations:\n",
        "\n",
        "$$\n",
        "\\mathbf{z}^{(L)} = \\mathbf{W}^{(L)} \\mathbf{a}^{(L-1)} + \\mathbf{b}^{(L)} \\\\\n",
        "\\mathbf{a}^{(L)} = \\sigma(\\mathbf{z}^{(L)}) \\\\\n",
        "C = \\sum_i (a_i^{(L)} - y_i)^2\n",
        "$$\n",
        "\n",
        "So that using the chain rule, we can write:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial{C}}{\\partial{W^{(L)}}} = \\frac{\\partial{C}}{\\partial{a^{(L)}}} \\frac{\\partial{a^{(L)}}}{\\partial{z^{(L)}}} \\frac{\\partial{z^{(L)}}}{\\partial{W^{(L)}}} \\\\\n",
        "\\frac{\\partial{C}}{\\partial{b^{(L)}}} = \\frac{\\partial{C}}{\\partial{a^{(L)}}} \\frac{\\partial{a^{(L)}}}{\\partial{z^{(L)}}} \\frac{\\partial{z^{(L)}}}{\\partial{b^{(L)}}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e8ed97a",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
