{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b",
   "metadata": {
    "id": "7cf0aedd-9400-4ce1-8885-d800f997cb6b"
   },
   "source": [
    "# Chapters 4 and 5 -- ArgMax, SoftMax, and Cross Entropy\n",
    "\n",
    "In this tutorial, we will use **[PyTorch](https://pytorch.org/) and [Lightning](https://www.lightning.ai/)** to create and optimize a simple neural network with multiple inputs and outputs that uses the `SoftMax()` function during training, the the one in the picture shown below.\n",
    "\n",
    "<img src=\"./images/chapter_4_softmax.png\" alt=\"a neural network with multiple inputs and outputs and softmax\" style=\"width: 800px;\">\n",
    "\n",
    "and the `ArgMax()` function during inference.\n",
    "\n",
    "<img src=\"./images/chapter_4_argmax.png\" alt=\"a neural network with multiple inputs and outputs and argmax\" style=\"width: 800px;\">\n",
    "\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "- Import and Format Data and then Build a DataLoader From Scratch\n",
    "- Build a Neural Network with Multiple Inputs and Outputs and the SoftMax() function\n",
    "- Train a Neural Network with Multiple Inputs and Outputs and the SoftMax() function\n",
    "- Make Predictions with New Data Using the Trained Model and the ArgMax() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {
    "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383057be-98c7-4741-b3cd-20aa54a94d9a",
   "metadata": {
    "id": "383057be-98c7-4741-b3cd-20aa54a94d9a"
   },
   "source": [
    "## Build a DataLoader\n",
    "\n",
    "As in the previous chapter, we will use the **[Iris flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)**, which we will import from a comma-separated (CSV) text file. We will then proceed exactly the same, and prepare the training data into a **DataLoader**, which we can use to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a23b9c-ae55-41cd-9828-db0f0eb39e3d",
   "metadata": {
    "id": "46a23b9c-ae55-41cd-9828-db0f0eb39e3d"
   },
   "outputs": [],
   "source": [
    "# --- Import the scaler and label encoder from sklearn ---\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Load and Define X and y ---\n",
    "path = \"./iris.txt\"\n",
    "# Assign column names since the file has no header\n",
    "column_names = [\n",
    "    'sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    path, sep=\",\", names=column_names)\n",
    "\n",
    "# --- Define X and y ---\n",
    "input_values = df[['petal_width', 'sepal_width']]\n",
    "label_values = df['class']\n",
    "\n",
    "# --- SPLIT THE DATA FIRST ---\n",
    "# (This is best practice)\n",
    "input_train, input_test, label_train_str, label_test_str = train_test_split(\n",
    "    input_values,\n",
    "    label_values,           # split the original label strings\n",
    "    test_size=0.25,\n",
    "    stratify=label_values,  # stratify on the label strings\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- SKLEARN PREPROCESSING ---\n",
    "# Initialize and fit the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(label_train_str)  # Fit *only* on the training labels\n",
    "\n",
    "# Transform *both* sets\n",
    "label_train_num = le.transform(label_train_str)\n",
    "label_test_num = le.transform(label_test_str)\n",
    "\n",
    "# (We can also use le.fit_transform(label_train_str) for the first one)\n",
    "\n",
    "# Initialize and fit the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(input_train)\n",
    "input_train_scaled = scaler.transform(input_train)\n",
    "input_test_scaled = scaler.transform(input_test)\n",
    "\n",
    "# --- PYTORCH CONVERSION ---\n",
    "# Convert scaled data to tensors\n",
    "input_train_tensors = torch.tensor(input_train_scaled, dtype=torch.float32)\n",
    "input_test_tensors = torch.tensor(input_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Convert encoded labels to tensors\n",
    "label_train_tensors = torch.tensor(label_train_num)\n",
    "label_test_tensors = torch.tensor(label_test_num)\n",
    "\n",
    "# One-hot encode the numeric labels\n",
    "# The model's output layer will produce float32 predictions. \n",
    "# To calculate the loss, PyTorch requires the predictions \n",
    "# and the labels to be the same data type.\n",
    "one_hot_label_train = F.one_hot(label_train_tensors).type(torch.float32)\n",
    "one_hot_label_test = F.one_hot(label_test_tensors).type(torch.float32)\n",
    "\n",
    "# Create TensorDataset and DataLoader as before\n",
    "train_dataset = TensorDataset(input_train_tensors, one_hot_label_train)\n",
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea",
   "metadata": {
    "id": "98ed8a32-4aeb-4a4e-9c6c-4bfff11ba0ea"
   },
   "source": [
    "## Building a neural network\n",
    "\n",
    "Building a neural network with PyTorch means creating a new class. And to make it easy to train the neural network, this class will inherit from `LightningModule`.\n",
    "\n",
    "Our new class will have the following methods:\n",
    "\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other housekeeping things.\n",
    "- `forward()` to make a forward pass through the neural network.\n",
    "- `configure_optimizers()` to configure the optimizer. There are lots of optimizers to choose from, but in this tutorial, we'll change things up and use `Adam`.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and keep track of the loss values in a log file.\n",
    "\n",
    "Also, for reference, here is a picture of the neural network we want to create is more completed than in the previous chapter, in particular with the addition of a `SoftMax` function in the output layer, and therefore the need for another cost function:\n",
    "\n",
    "<img src=\"./images/chapter_4_softmax.png\" alt=\"a neural network with multiple inputs and outputs and softmax\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f26851-96df-4a84-b1ce-3351e61adf25",
   "metadata": {
    "id": "73f26851-96df-4a84-b1ce-3351e61adf25"
   },
   "source": [
    "As we can see in the picture, our neural network has 2 inputs, one for Petal Width and one for Sepal Width, a single hidden layer with two **[ReLU](https://youtu.be/68BZ5f7P94E)** activation functions, and 3 outputs, one for each species of iris.\n",
    "\n",
    "So, given this specification for this neural network, let's code it in a new class called `MultipleInsOuts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611711c1-ea20-48fa-bb89-c2ce5074dde7",
   "metadata": {
    "id": "611711c1-ea20-48fa-bb89-c2ce5074dde7"
   },
   "outputs": [],
   "source": [
    "class MultipleInsOuts(L.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        L.seed_everything(seed=42)\n",
    "\n",
    "        # We can set the net that will be used by the forward() method here,\n",
    "        # also the default for the bias argument is True\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=2),  # Linear\n",
    "            nn.ReLU(),                                 # ReLU\n",
    "            nn.Linear(in_features=2, out_features=3),  # Linear\n",
    "        )\n",
    "\n",
    "        # We'll use Cross Entropy to calculate the loss between what the\n",
    "        # neural network's predictions and actual, or known, species for\n",
    "        # each row in the dataset.\n",
    "        # NOTE: nn.CrossEntropyLoss applies a SoftMax function to the values\n",
    "        # we give it, so we don't have to do that oursevles. However,\n",
    "        # when we use this neural network (after it has been trained), we'll\n",
    "        # have to remember to apply a SoftMax function to the output.\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # We change the method slightly compared to the previous chapter\n",
    "\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        return Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        inputs, labels = batch\n",
    "        outputs = self.forward(inputs)\n",
    "        loss = self.loss(outputs, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb353a83-3690-4a36-a164-e60e499bc79f",
   "metadata": {
    "id": "cb353a83-3690-4a36-a164-e60e499bc79f"
   },
   "source": [
    "## Training the Neural Network\n",
    "\n",
    "Training our new neural network means we create a model from the new class, `MultipleInsOuts` and then create a **Lightning Trainer**, `L.Trainer`, and use it to optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1c858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.weight',\n",
       "              tensor([[ 0.5406,  0.5869],\n",
       "                      [-0.1657,  0.6496]])),\n",
       "             ('net.0.bias', tensor([-0.1549,  0.1427])),\n",
       "             ('net.2.weight',\n",
       "              tensor([[-0.3443,  0.4153],\n",
       "                      [ 0.6233, -0.5188],\n",
       "                      [ 0.6146,  0.1323]])),\n",
       "             ('net.2.bias', tensor([0.5224, 0.0958, 0.3410]))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultipleInsOuts()\n",
    "\n",
    "# Let's print the parameters of the neural network\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa986643",
   "metadata": {},
   "source": [
    "We already see that the neural network initiated here is the same as in the previous chapter, because we use the same configuration with the same seed number. What changes really is the output throught the SoftMax function, and therefore the training process that requires another cost function for minimization.\n",
    "\n",
    "We will start with 10 epochs, complete runs through our training data. This may be enough to successfully optimize all of the parameters, but it might not. We'll find out later in the tutorial when we make a graph of how the loss values change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d",
   "metadata": {
    "id": "bee5f1bc-fcef-4cac-b33e-1225ec246f2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | net  | Sequential       | 15     | train\n",
      "1 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "15        Trainable params\n",
      "0         Non-trainable params\n",
      "15        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277a52913054444187a760337fbd8e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f",
   "metadata": {
    "id": "2dc0b74a-127c-49aa-8168-91dd28483b5f"
   },
   "source": [
    "The table differs slightly from the previous chapter, though the information are similar. Now the layers are not separated but grouped into a 'net'. Also, the loss function reflects that we are using a SoftMax function, which necessitates 'CrossEntropyLoss'.\n",
    "\n",
    "Now, let's see if the predictions are any good. We can do this by seeing how well it predicts the testing data. We'll start by running `input_test_tensors` through the neural network and saving the output `predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcdcfd9-36f1-44d2-ae8c-3b3f78e99693",
   "metadata": {
    "id": "0fcdcfd9-36f1-44d2-ae8c-3b3f78e99693"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9644, -0.1377,  0.0748],\n",
       "        [ 0.0507,  0.5664,  0.8702],\n",
       "        [ 0.1509,  0.4944,  0.6980],\n",
       "        [ 0.1030,  0.5353,  0.6764]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the input_test_tensors through the neural network\n",
    "predictions = model(input_test_tensors)\n",
    "predictions[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52512e0f-9a49-4080-aeea-ceac0745ca5a",
   "metadata": {
    "id": "52512e0f-9a49-4080-aeea-ceac0745ca5a"
   },
   "source": [
    "We can determine which species was predicted in `predictions` by selecting the index in each row that corresponding to the largest value, and we do that with `torch.argmax()`. `torch.argmax()` returns a tensor that contains the indices with the largest values for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a336e70-1e01-48a2-9761-cb51c845dbbb",
   "metadata": {
    "id": "8a336e70-1e01-48a2-9761-cb51c845dbbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the output with highest value...\n",
    "predicted_labels = torch.argmax(predictions, dim=1) ## dim=0 applies argmax to rows, dim=1 applies argmax to columns\n",
    "predicted_labels[0:4] # print out the first 4 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb622159-b415-4e2e-83af-328cabad438e",
   "metadata": {
    "id": "bb622159-b415-4e2e-83af-328cabad438e"
   },
   "source": [
    "In the first row index 0 had the largest value. Thus, the first prediction corresponds to **Setosa**. The second, third, and fourth rows predicted 2, which corresponds to **Virginica**.\n",
    "\n",
    "Now, let's compare what the neural network predicted in `predicted_labels` to the known values in `label_test` and calculate the percentage of correct predictions. We do this by adding up the number of times an element in `predicted_labels` equals the corresponding element in `label_test` and dividing by the number of elements in `predicted_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0872e381-57ef-43e3-a6b4-9aa9d96c93e4",
   "metadata": {
    "id": "0872e381-57ef-43e3-a6b4-9aa9d96c93e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6579)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(\n",
    "    torch.eq(\n",
    "        label_test_tensors,\n",
    "        predicted_labels\n",
    "    )) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd75320-d89e-4928-84ba-9b3baaa966c5",
   "metadata": {
    "id": "2dd75320-d89e-4928-84ba-9b3baaa966c5"
   },
   "source": [
    "And we see that our neural network only correctly predicts 65.8% of the testing data. This isn't very good. So, let's add an additional **90** epochs to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a849d2f-bad1-4214-a72b-f1b04174639c",
   "metadata": {
    "id": "5a849d2f-bad1-4214-a72b-f1b04174639c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restoring states from the checkpoint path at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_04\\lightning_logs\\version_9\\checkpoints\\epoch=9-step=1120.ckpt\n",
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:445: The dirpath has changed from 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_04\\\\lightning_logs\\\\version_9\\\\checkpoints' to 'c:\\\\Users\\\\SÃ©bastien\\\\Documents\\\\data_science\\\\machine_learning\\\\statsquest_neural_networks\\\\chapter_04\\\\lightning_logs\\\\version_10\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name | Type             | Params | Mode \n",
      "--------------------------------------------------\n",
      "0 | net  | Sequential       | 15     | train\n",
      "1 | loss | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------\n",
      "15        Trainable params\n",
      "0         Non-trainable params\n",
      "15        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\chapter_04\\lightning_logs\\version_9\\checkpoints\\epoch=9-step=1120.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee4d50b2f9d4de6bae1756015de31ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "# First, create a new Lightning Trainer\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    ckpt_path=path_to_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e",
   "metadata": {
    "id": "7ca3ecd4-f56d-4a1e-89cc-1f7cf5992a1e"
   },
   "source": [
    "Now, let's run the testing data through the network and calculate the accuracy. We'll do this just like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "930f26fe-eca7-46a3-9652-0e1d1b1bcfb6",
   "metadata": {
    "id": "930f26fe-eca7-46a3-9652-0e1d1b1bcfb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9474)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = torch.argmax(\n",
    "    model(input_test_tensors),\n",
    "    dim=1)\n",
    "\n",
    "torch.sum(\n",
    "    torch.eq(\n",
    "        label_test_tensors,\n",
    "        predicted_labels\n",
    "    )) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31918cf1-5cfc-475b-b61a-f08525111741",
   "metadata": {
    "id": "31918cf1-5cfc-475b-b61a-f08525111741"
   },
   "source": [
    "After 100 training epochs, we correctly classified almost 95% of the testing data using the NN configured as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79958e71-548a-49a6-a6c5-d2a5ddd279c4",
   "metadata": {
    "id": "79958e71-548a-49a6-a6c5-d2a5ddd279c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('net.0.weight',\n",
       "              tensor([[ 3.5468,  0.2525],\n",
       "                      [-1.9557,  1.4759]])),\n",
       "             ('net.0.bias', tensor([-0.3422,  1.2418])),\n",
       "             ('net.2.weight',\n",
       "              tensor([[-4.2038,  3.1493],\n",
       "                      [ 0.1572,  0.3311],\n",
       "                      [ 1.8735, -3.2946]])),\n",
       "             ('net.2.bias', tensor([ 0.4556,  0.9597, -0.6117]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c900f6",
   "metadata": {},
   "source": [
    "Remind that in PyTorch's `nn.Linear` the weight matrix is always structured as `[number_of_outputs, number_of_inputs]`. Therefore:\n",
    "\n",
    "- each ROW in the tensor corresponds to one output node\n",
    "- each COLUMN in the tensor corresponds to one input node\n",
    "\n",
    "For example:\n",
    "\n",
    "```text\n",
    "Inputs:\n",
    "           col 0 (from I0)   col 1 (from I1)\n",
    "        [[   3.5468       ,     0.2525      ],  <-- row 0 (to H0)\n",
    "         [  -1.9557       ,     1.4759      ]]  <-- row 1 (to H1)\n",
    "Outputs:\n",
    "```\n",
    "\n",
    "For the weights between the hidden and the output layers:\n",
    "\n",
    "```text\n",
    "col 0 (from H0)   col 1 (from H1)\n",
    "        [[  -4.2038       ,     3.1493      ],  <-- row 0 (to O0)\n",
    "         [   0.1572       ,     0.3311      ],  <-- row 1 (to O1)\n",
    "         [   1.8735       ,    -3.2946      ]]  <-- row 2 (to O2)\n",
    "```\n",
    "\n",
    "This corresponds to the following NN configuration.\n",
    "\n",
    "```mermaid\n",
    "%%{init:{'theme': 'neutral'}}%%\n",
    "graph LR\n",
    "    %% Style Definitions\n",
    "    classDef biasStyle fill:none,stroke:none,color:#7f8c8d;\n",
    "\n",
    "    %% Input Layer (2 nodes)\n",
    "    subgraph Input Layer [Inputs]\n",
    "        direction TB\n",
    "        I0((\"Iâ‚€\"))\n",
    "        I1((\"Iâ‚\"))\n",
    "    end\n",
    "\n",
    "    %% Hidden Layer (2 nodes)\n",
    "    subgraph Hidden Layer [\"Hidden (ReLU)\"]\n",
    "        direction TB\n",
    "        H0((\"Hâ‚€\"))\n",
    "        H1((\"Hâ‚\"))\n",
    "    end\n",
    "\n",
    "    %% Output Layer (3 nodes)\n",
    "    subgraph Output Layer [Outputs]\n",
    "        direction TB\n",
    "        O0((\"Oâ‚€\"))\n",
    "        O1((\"Oâ‚\"))\n",
    "        O2((\"Oâ‚‚\"))\n",
    "    end\n",
    "\n",
    "    %% Bias Nodes for Hidden Layer\n",
    "    BH0[\"Bias: -0.34\"]:::biasStyle\n",
    "    BH1[\"Bias: 1.24\"]:::biasStyle\n",
    "\n",
    "    %% Bias Nodes for Output Layer\n",
    "    BO0[\"Bias: 0.46\"]:::biasStyle\n",
    "    BO1[\"Bias: 0.96\"]:::biasStyle\n",
    "    BO2[\"Bias: -0.61\"]:::biasStyle\n",
    "\n",
    "    %% Connections from Input to Hidden Layer Weights\n",
    "    I0 -->|\"3.55\"| H0\n",
    "    I1 -->|\"0.25\"| H0\n",
    "\n",
    "    I0 -->|\"-1.96\"| H1\n",
    "    I1 -->|\"1.48\"| H1\n",
    "\n",
    "    %% Connections from Hidden Layer Biases\n",
    "    BH0 -.-> H0\n",
    "    BH1 -.-> H1\n",
    "\n",
    "    %% Connections from Hidden to Output Layer Weights\n",
    "    H0 -->|\"-4.20\"| O0\n",
    "    H1 -->|\"3.15\"| O0\n",
    "\n",
    "    H0 -->|\"0.16\"| O1\n",
    "    H1 -->|\"0.33\"| O1\n",
    "\n",
    "    H0 -->|\"1.87\"| O2\n",
    "    H1 -->|\"-3.29\"| O2\n",
    "\n",
    "    %% Connections from Output Layer Biases\n",
    "    BO0 -.-> O0\n",
    "    BO1 -.-> O1\n",
    "    BO2 -.-> O2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098",
   "metadata": {
    "id": "5c2df2a0-77e3-40dd-8cf5-3a23d66e5098"
   },
   "source": [
    "## Make a prediction\n",
    "\n",
    "Now that our model is trained, we can use it to make predictions from new data. This is done by passing the model a tensor with normalized petal and sepal widths wrapped up in a tensor, and then passing the model output to `torch.argmax()`.\n",
    "\n",
    "For example, if the raw petal and sepal width measurements were 0.2 and 3.0, we would first normalize them using the scaler fitted with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8759e954-3d0b-4ac3-84ac-5c294585643a",
   "metadata": {
    "id": "8759e954-3d0b-4ac3-84ac-5c294585643a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petal and sepal widths =  [[0.2 3. ]]\n",
      "Normalized petal and sepal widths =  [[0.04166667 0.41666667]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SÃ©bastien\\Documents\\data_science\\machine_learning\\statsquest_neural_networks\\.env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "petal_sepal_widths = np.array([.2, 3]).reshape(1, -1)\n",
    "print(\"Petal and sepal widths = \", petal_sepal_widths)\n",
    "\n",
    "normalized_petal_sepal_widths = scaler.transform(petal_sepal_widths)\n",
    "print(\"Normalized petal and sepal widths = \", normalized_petal_sepal_widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e1e78-d0dd-46c8-9cd5-f59815385dea",
   "metadata": {
    "id": "211e1e78-d0dd-46c8-9cd5-f59815385dea"
   },
   "source": [
    "Then we convert `normalized_values` into a tensor and pass it to the model, and pass the output to the `argmax()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "192cb03b-44f5-4fcf-961d-426a79d40c7d",
   "metadata": {
    "id": "192cb03b-44f5-4fcf-961d-426a79d40c7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(\n",
    "    model(\n",
    "        torch.tensor(normalized_petal_sepal_widths).type(torch.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053241f-37bb-44b3-b2da-38e841679422",
   "metadata": {
    "id": "2053241f-37bb-44b3-b2da-38e841679422"
   },
   "source": [
    "And the output is 0, meaning that the neural network predicts that the measurements come from **Setosa**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8fc18",
   "metadata": {},
   "source": [
    "## Training with the GPU\n",
    "\n",
    "Using an Intel Iris Xe GPU on my Surface Laptop, we can actually take advantage of it without installing any additional extension, as [explained elsewhere](https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74971ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.xpu.is_available())  # torch.xpu is the API for Intel GPU support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80bce6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 XPU device(s):\n",
      "[0]: _XpuDeviceProperties(name='Intel(R) Iris(R) Xe Graphics', platform_name='Intel(R) oneAPI Unified Runtime over Level-Zero', type='gpu', device_id=0x9A49, uuid=8680499a-0100-0000-0002-000000000000, driver_version='1.6.34728', total_memory=14876MB, max_compute_units=96, gpu_eu_count=96, gpu_subslice_count=12, max_work_group_size=512, max_num_sub_groups=64, sub_group_sizes=[8 16 32], has_fp16=1, has_fp64=0, has_atomic64=1)\n"
     ]
    }
   ],
   "source": [
    "# Check if any XPU devices exist first\n",
    "device_count = torch.xpu.device_count()\n",
    "\n",
    "if device_count == 0:\n",
    "    print(\"No XPU devices found.\")\n",
    "else:\n",
    "    print(f\"Found {device_count} XPU device(s):\")\n",
    "    # Iterate with a standard for loop\n",
    "    for i in range(device_count):\n",
    "        print(f\"[{i}]: {torch.xpu.get_device_properties(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f79b0",
   "metadata": {},
   "source": [
    "Now we confirmed GPU is recognized by PyTorch, we can repeat the training using the same model but on another 'device', the DataLoader staying on the CPU. All we would have to do is to configure the 'Trainer' to use the GPU. The `L.Trainer(accelerator=\"gpu\")` is a high-level command that automates all steps when we call `trainer.fit()`.\n",
    "\n",
    "However, the current version of `lightning` is [not able to use the Intel GPU accelerator](https://github.com/Lightning-AI/pytorch-lightning/issues/20938). But we can take all the individual pieces we defined in the LightningModule and manually arrange them into a \"pure PyTorch\" training loop. For that, we need to create a new class of model.\n",
    "\n",
    "First of all, we need to go back one step further, and review the specifications of the DataLoader. Because we haven't provided any btach size, the PyTorch DataLoader used its default, which is `batch_size=1`. Therefore, the model trained using **Stochastic Gradient Descent** (updating weights after every single sample). This is very noisy but can sometimes learn. By setting the argument `batch_size=16`, the model trully trains using **Mini-Batch Gradient Descent** (updating weights 7 times per epoch, or 112/16). We can also try a smaller batch_size (like 8) to create smaller, shorter compute jobs for the GPU, which might prevent it from overheating or timing out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e340f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_gpu = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# We also need the test loader for evaluation\n",
    "test_dataset = TensorDataset(input_test_tensors, label_test_tensors)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c3bcd",
   "metadata": {},
   "source": [
    "Note that the `train_dataloader_gpu` has 112 samples. If our batch_size is 16, this divides perfectly (112 / 16 = 7). The Intel driver \"warmed up\" by compiling highly optimized code (kernels) for batches of 16. However, the `test_dataloader` has 38 samples, and after the second batch of 16, there is a leftover of 6 samples, and we don't have a compiled kernel for this new, different mismatched shape.\n",
    "\n",
    "The solution is to add a new `drop_last=True` argument to the definition of the DataLoaders to tell the `test_dataloader` to ignore that last, incomplete batch. This is a standard practice when working with JIT-compilers (like on XPUs or TPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caad80e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_gpu = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a0cef4",
   "metadata": {},
   "source": [
    "We redefine the model so that it fits the hard PyTorch code. Note that the new class doesn't inherit from `L.LightningModule`, and does not define the optimizer and the training step, like the previous one. The new `nn.Module` class is now just a \"blueprint\" for the network architecture (the `__init__`) and the data flow (the `forward` pass), all the training logic has been moved out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dbc7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleInsOutsGPU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        L.seed_everything(seed=42)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2, out_features=3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc2a516",
   "metadata": {},
   "source": [
    "Next step is defining the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d90fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"xpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e19b5",
   "metadata": {},
   "source": [
    "Finally, we instantiate the model, loss and optimizer as follows.\n",
    "\n",
    "Note that, the loss would identical when the model's weights are not being updated. This happens when we create the optimizer before we move the model to the GPU. The optimizer locks onto the specific set of parameters (weights and biases) we give it when it's created, trying to update the original model (on the CPU), which will be never used. Therefore, we must create the optimizer **after** we have moved the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc17db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating training on xpu for 100 epochs...\n"
     ]
    }
   ],
   "source": [
    "# Define epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Instantiate model\n",
    "model_gpu = MultipleInsOutsGPU()\n",
    "\n",
    "# Move model to the \"xpu\" device IN-PLACE\n",
    "model_gpu.to(device)\n",
    "\n",
    "# Instantiate loss and optimizer afterwards\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = Adam(model_gpu.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Initiating training on {device} for {NUM_EPOCHS} epochs...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda03e37",
   "metadata": {},
   "source": [
    "When we call `model.to(\"xpu\")` for the very first time, we are not just moving weights, we are triggering a heavy, one-time initialization process for the entire Intel GPU backend that involves:\n",
    "\n",
    "- Initializing the oneAPI/Level Zero Runtime: PyTorch has to \"wake up\" the entire driver stack.\n",
    "- JIT compilation: this is the main one. The Intel driver (via IPEX or the new PyTorch 2.9 backend) is Just-In-Time compiling the specific GPU code (kernels) needed for the operations in your model (like nn.Linear, nn.ReLU, etc.)\n",
    "\n",
    "Another issue we might encounter is the GPU is no longer responding for different reasons, such as obverheating, driver crash, or timeout in Windows power management. Also, we may loss information in a Notebook when we re-run the training/testing process.\n",
    "\n",
    "Therefore it's much cleaner to just train for 100 epochs from the start. It doesn't take much longer to run 100 vs. 10 epochs, the longest waiting time is during the establishment of the connection with the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80cec6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultipleInsOutsGPU(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=2, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set model to training mode\n",
    "model_gpu.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34202852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.0169\n",
      "Epoch [20/100], Loss: 0.9831\n",
      "Epoch [30/100], Loss: 0.9466\n",
      "Epoch [40/100], Loss: 0.9069\n",
      "Epoch [50/100], Loss: 0.8652\n",
      "Epoch [60/100], Loss: 0.8232\n",
      "Epoch [70/100], Loss: 0.7818\n",
      "Epoch [80/100], Loss: 0.7419\n",
      "Epoch [90/100], Loss: 0.7046\n",
      "Epoch [100/100], Loss: 0.6702\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_gpu:\n",
    "        \n",
    "        # Move data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        outputs = model_gpu(inputs)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 3. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print loss once per 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        avg_loss = epoch_loss / len(train_dataloader_gpu)\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421198f1",
   "metadata": {},
   "source": [
    "Let's evaluate the GPU model after 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45a418c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the 32 test samples: 68.75 %\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model_gpu.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # We don't need to calculate gradients\n",
    "    for inputs, labels in test_dataloader:\n",
    "        # Move test data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model_gpu(inputs)\n",
    "        \n",
    "        # Get the prediction (index of the max logit)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "        # 1. Compare 'predicted' and 'labels' on the GPU\n",
    "        comparison = (predicted == labels)\n",
    "        \n",
    "        # 2. MOVE THE *ENTIRE* BOOLEAN TENSOR TO THE CPU *FIRST*\n",
    "        comparison_on_cpu = comparison.to(\"cpu\")\n",
    "        \n",
    "        # 3. Now, sum the tensor and get the value *on the CPU*.\n",
    "        #    The CPU knows how to sum a boolean tensor.\n",
    "        correct += comparison_on_cpu.sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the {total} test samples: {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeec22",
   "metadata": {},
   "source": [
    "The accuracy after 100 epochs on the GPU is very similar to the accuracy after 10 epochs we obtained from the model with Lightning on the CPU.\n",
    "\n",
    "The good news is that because we are using a pure PyTorch loop, our GPU model and optimizer objects are still in memory, holding their trained state from the first 100 epochs. To add 900 more epochs, we just need to run the training loop cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7aba72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.4679\n",
      "Epoch [200/1000], Loss: 0.3822\n",
      "Epoch [300/1000], Loss: 0.3217\n",
      "Epoch [400/1000], Loss: 0.2621\n",
      "Epoch [500/1000], Loss: 0.1900\n",
      "Epoch [600/1000], Loss: 0.1536\n",
      "Epoch [700/1000], Loss: 0.1318\n",
      "Epoch [800/1000], Loss: 0.1180\n",
      "Epoch [900/1000], Loss: 0.1085\n",
      "Epoch [1000/1000], Loss: 0.1018\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1000\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    for inputs, labels in train_dataloader_gpu:\n",
    "        \n",
    "        # Move data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        outputs = model_gpu(inputs)\n",
    "        \n",
    "        # 2. Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 3. Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print loss once every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        avg_loss = epoch_loss / len(train_dataloader_gpu)\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "911d9dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the 32 test samples: 93.75 %\n"
     ]
    }
   ],
   "source": [
    "# Set model to evaluation mode\n",
    "model_gpu.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # We don't need to calculate gradients\n",
    "    for inputs, labels in test_dataloader:\n",
    "        # Move test data to the device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = model_gpu(inputs)\n",
    "        \n",
    "        # Get the prediction (index of the max logit)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "\n",
    "        # 1. Compare 'predicted' and 'labels' on the GPU\n",
    "        comparison = (predicted == labels)\n",
    "        \n",
    "        # 2. MOVE THE *ENTIRE* BOOLEAN TENSOR TO THE CPU *FIRST*\n",
    "        comparison_on_cpu = comparison.to(\"cpu\")\n",
    "        \n",
    "        # 3. Now, sum the tensor and get the value *on the CPU*.\n",
    "        #    The CPU knows how to sum a boolean tensor.\n",
    "        correct += comparison_on_cpu.sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on the {total} test samples: {accuracy:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab0fc8d",
   "metadata": {},
   "source": [
    "After 1000 training epochs, we correctly classified almost 93.75% of the testing data using the NN configured as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef9ef37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Current Model Parameters ---\n",
      "Layer: net.0.weight\n",
      "Values:\n",
      "tensor([[ 3.5035,  0.7938],\n",
      "        [-2.6496,  2.0173]])\n",
      "\n",
      "Layer: net.0.bias\n",
      "Values:\n",
      "tensor([-0.6270,  1.5825])\n",
      "\n",
      "Layer: net.2.weight\n",
      "Values:\n",
      "tensor([[-4.5367,  3.7649],\n",
      "        [-0.2842,  0.2392],\n",
      "        [ 3.0659, -3.9450]])\n",
      "\n",
      "Layer: net.2.bias\n",
      "Values:\n",
      "tensor([-0.8396,  2.2043, -1.5938])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode first (good practice)\n",
    "model_gpu.eval()\n",
    "\n",
    "print(\"--- Current Model Parameters ---\")\n",
    "\n",
    "# Loop through each parameter (weights and biases)\n",
    "for name, param in model_gpu.named_parameters():\n",
    "    \n",
    "    # We move the data to the CPU for clean printing\n",
    "    # and use .data to detach it from the graph\n",
    "    param_data_cpu = param.data.to(\"cpu\")\n",
    "    \n",
    "    print(f\"Layer: {name}\\nValues:\\n{param_data_cpu}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933fc327",
   "metadata": {},
   "source": [
    "Finally we could think about defining functions for training, updating and evaluating the GPU models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "304b9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains the model for a given number of epochs.\n",
    "    Note: This function modifies the model and optimizer in-place.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting training for {num_epochs} epoch(s) on {device} ---\")\n",
    "    \n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            # Move data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 3. Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Print loss at the end of the training epochs\n",
    "        # (We'll print every 10 epochs, or at the end if fewer than 10)\n",
    "        if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs:\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            print(f\"  Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Training finished. ---\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on the test dataset.\n",
    "    \"\"\"\n",
    "    print(f\"--- Evaluating model on {device} ---\")\n",
    "    \n",
    "    # Set model to evaluation mode (e.g., turns off dropout)\n",
    "    model.eval() \n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # We don't need to calculate gradients during evaluation\n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_loader:\n",
    "            \n",
    "            # Move test data to the device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get the prediction (index of the max logit)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # --- Use the CPU workaround for the .sum() bug ---\n",
    "            comparison = (predicted == labels)\n",
    "            comparison_on_cpu = comparison.to(\"cpu\")\n",
    "            correct += comparison_on_cpu.sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"  Accuracy on the {total} test samples: {accuracy:.2f} %\")\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
